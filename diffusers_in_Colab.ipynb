{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/suzukimain/diffusers_in_Colab/blob/main/diffusers_in_Colab.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eicnuls7qg7l"
      },
      "outputs": [],
      "source": [
        "#@title   (option)ドライブに接続 { run: \"auto\", display-mode: \"form\"}\n",
        "Google_driveに接続 = True  # @param {type:\"boolean\"}\n",
        "\n",
        "conect_drive=False\n",
        "\n",
        "from google.colab import drive\n",
        "if Google_driveに接続:\n",
        "    Gdrive=\"GoogleDrive: \\033[32m接続成功\\033[0m\"\n",
        "    if not drive._os.path.ismount('/content/drive'):\n",
        "        try:\n",
        "            drive.mount('/content/drive')\n",
        "            conect_drive=True\n",
        "        except:\n",
        "            Gdrive=\"GoogleDrive: \\033[31m接続失敗\\033[0m\"\n",
        "            conect_drive=False\n",
        "else:\n",
        "    Gdrive=\"GoogleDrive: \\033[33m接続なし\\033[0m\"\n",
        "    if drive._os.path.ismount('/content/drive'):\n",
        "        drive.flush_and_unmount()\n",
        "        conect_drive=False\n",
        "        print(\"GoogleDriveの接続を解除しました\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "C4UJqC_WpaGw"
      },
      "outputs": [],
      "source": [
        "#@title   #Step.1 セットアップ (Setup) {display-mode: \"form\"}\n",
        "print(\"実行中…\")\n",
        "#if torch.cuda.device_count()==False:\n",
        "#  raise RuntimeError(\"deviceをGPUに変更お願いします\")\n",
        "\n",
        "開発版のライブラリを使用 = True  #@param {type:\"boolean\"}\n",
        "#@markdown ※バグが起きる可能性あり\n",
        "\n",
        "import torch,os\n",
        "from importlib import reload\n",
        "from google.colab import drive\n",
        "import locale #この2行は非常に重要\n",
        "locale.getpreferredencoding = lambda: \"UTF-8\"\n",
        "\n",
        "\n",
        "if not drive._os.path.ismount('/content/drive'):\n",
        "    conect_drive=False\n",
        "else:\n",
        "    conect_drive=True\n",
        "\n",
        "\n",
        "if 開発版のライブラリを使用:\n",
        "    !pip install --upgrade -q git+https://github.com/huggingface/diffusers.git\n",
        "\n",
        "else:\n",
        "    !pip install --upgrade -q diffusers\n",
        "\n",
        "import diffusers\n",
        "reload(diffusers)\n",
        "!pip install --upgrade -q transformers omegaconf  sacremoses googletrans==3.1.0a0\n",
        "\n",
        "import torch\n",
        "import transformers\n",
        "import safetensors\n",
        "import huggingface_hub\n",
        "import omegaconf\n",
        "import glob\n",
        "import gc\n",
        "import time\n",
        "import sys\n",
        "import requests\n",
        "import urllib.request\n",
        "import codecs\n",
        "import re\n",
        "import pickle\n",
        "import torch\n",
        "import random\n",
        "import numpy as np\n",
        "import googletrans\n",
        "import ipywidgets as widgets\n",
        "from PIL import Image,PngImagePlugin\n",
        "from requests import HTTPError\n",
        "import difflib\n",
        "import importlib\n",
        "import inspect\n",
        "import datetime\n",
        "from IPython.display import display, Markdown\n",
        "from diffusers import (StableDiffusionPipeline,\n",
        "                       StableDiffusionImg2ImgPipeline,\n",
        "                       StableDiffusionDepth2ImgPipeline,\n",
        "                       DPMSolverMultistepScheduler,\n",
        "                       AutoencoderKL,\n",
        "                       PNDMScheduler,\n",
        "                       schedulers,\n",
        "                       TextToVideoZeroPipeline,\n",
        "                       StableDiffusionInpaintPipeline,\n",
        "                       StableDiffusionPipelineSafe,\n",
        "                       DDPMScheduler,\n",
        "                       DDIMScheduler,\n",
        "                       PNDMScheduler,\n",
        "                       LMSDiscreteScheduler,\n",
        "                       EulerDiscreteScheduler,\n",
        "                       EulerAncestralDiscreteScheduler,\n",
        "                       DPMSolverMultistepScheduler,\n",
        "                       DEISMultistepScheduler,\n",
        "                       UniPCMultistepScheduler,\n",
        "                       KDPM2DiscreteScheduler,\n",
        "                       DPMSolverSinglestepScheduler,\n",
        "                       KDPM2AncestralDiscreteScheduler,\n",
        "                       HeunDiscreteScheduler,\n",
        "                       )\n",
        "\n",
        "from transformers import (pipeline,\n",
        "                          CLIPTextModel,\n",
        "                          CLIPTokenizer,\n",
        "                          AutoModel,\n",
        "                          AutoTokenizer,\n",
        "                          AutoTokenizer,\n",
        "                          AutoModelForCausalLM)\n",
        "from diffusers.pipelines.stable_diffusion_safe import SafetyConfig\n",
        "from googletrans import Translator\n",
        "from IPython.display import display, HTML\n",
        "from huggingface_hub import hf_hub_download\n",
        "from torch import Generator\n",
        "import imageio\n",
        "from base64 import b64encode\n",
        "from google.colab import drive\n",
        "import google.colab.drive as drive\n",
        "from multiprocessing import Process, Manager\n",
        "import multiprocessing as mp\n",
        "\n",
        "\n",
        "from diffusers import logging as df_logging\n",
        "from transformers import logging as tf_logging\n",
        "import threading\n",
        "df_logging.set_verbosity_error()\n",
        "tf_logging.set_verbosity_error()\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "strength = 1\n",
        "入力する画像 = \"\"\n",
        "video_length= 8\n",
        "video_fps = 8\n",
        "safety_level = \"MAX\"\n",
        "momentum = 1\n",
        "momentum_hist = -0.1\n",
        "history_d = \"rand_init\"\n",
        "\n",
        "\n",
        "device=\"cuda\" if torch.cuda.is_available() else 'cpu'\n",
        "\n",
        "download_list=[]\n",
        "\n",
        "\n",
        "def key_check(keyword) -> bool:\n",
        "    global key_dict\n",
        "    if \"key_dict\" not in globals():\n",
        "        key_dict = {}\n",
        "    key = str(keyword)\n",
        "    key_in = False\n",
        "    if key in key_dict:\n",
        "        if keyword == key_dict[key]:\n",
        "          key_in = True\n",
        "    key_dict[key] = keyword\n",
        "    return key_in\n",
        "\n",
        "def check_url(url) -> bool:\n",
        "    flag = True\n",
        "    try:\n",
        "        f = urllib.request.urlopen(url)\n",
        "        f.close()\n",
        "    except:\n",
        "        flag = False\n",
        "    return flag\n",
        "\n",
        "\n",
        "\n",
        "step1_finish=True\n",
        "\n",
        "if conect_drive:\n",
        "    Connect_Gdrive=\"\\033[34mGoogleDrive: 接続成功\\033[0m\"\n",
        "else:\n",
        "    Connect_Gdrive=\"\\033[34mGoogleDrive: \\033[33m接続なし\\033[0m\"\n",
        "print(\"\\n\\033[34m___________________________________________\\033[0m\\n\")\n",
        "print(Connect_Gdrive)\n",
        "print(\"\\n\\033[32mセットアップが完了しました。\\033[0m\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2_TXXMSVDc5h"
      },
      "outputs": [],
      "source": [
        "#@title  #Step.2 モデル選択 (Model Selection){display-mode: \"form\"}\n",
        "\n",
        "#\"\\033[31m先にStep.1を実行してください\\033[0m\"\n",
        "\n",
        "\n",
        "# @markdown >使用するモデルの切り替え (model change)\n",
        "\n",
        "model_select = \"Counterfeit-V2.5(Anime)(better)\" # @param [\"stable diffusion-v2.1(basic)\", \"Counterfeit-V2.5(Anime)(better)\", \"loliDiffusion(Anime)\", \"waifu diffusion-v1.4(Anime)\", \"Anything-v3.0(Anime)\", \"Anything-v4.5(Anime)\", \"anything-midjourney-v-4-1(Anime)\", \"ACertainThing(Anime)\", \"anime-kawai-diffusion(Anime)\", \"AB4.5_AC0.2(Anime)\", \"basil_mix(Anime)\", \"Counterfeit(Anime)\", \"Counterfeit-V2.0(Anime)\", \"chilled_remix(Anime)\", \"Double-Exposure-Diffusion(Anime)\", \"EimisAnimeDiffusion_1.0v(Anime)\", \"7th_Layer(Anime)\", \"Riga_Collection(Anime)\", \"Waifu-Diffusers(Anime)\", \"JWST-Deep-Space-diffusion(space)\", \"sd-db-epic-space-machine(space_ship)\", \"spacemidj(space)\", \"nasa-space-v2(space)\", \"openjourney-v4(Reality)\", \"Realistic_Vision_V2.0(Reality)\", \"meinamix_meinaV10(Reality)\", \"search\"] {allow-input: true}\n",
        "del_word_list=[\"(basic)\",\"(Anime)\",\"(Reality)\",\"(space_ship)\",\"(space)\",\"(better)\"]\n",
        "if model_select in [\"stable diffusion-v2.1(basic)\", \"Counterfeit-V2.5(Anime)(better)\", \"loliDiffusion(Anime)\", \"waifu diffusion-v1.4(Anime)\", \"Anything-v3.0(Anime)\", \"Anything-v4.5(Anime)\", \"anything-midjourney-v-4-1(Anime)\", \"ACertainThing(Anime)\", \"anime-kawai-diffusion(Anime)\", \"AB4.5_AC0.2(Anime)\", \"basil_mix(Anime)\", \"Counterfeit(Anime)\", \"Counterfeit-V2.0(Anime)\", \"chilled_remix(Anime)\", \"Double-Exposure-Diffusion(Anime)\", \"EimisAnimeDiffusion_1.0v(Anime)\", \"7th_Layer(Anime)\", \"Riga_Collection(Anime)\", \"Waifu-Diffusers(Anime)\", \"JWST-Deep-Space-diffusion(space)\", \"sd-db-epic-space-machine(space_ship)\", \"spacemidj(space)\", \"nasa-space-v2(space)\", \"openjourney-v4(Reality)\", \"Realistic_Vision_V2.0(Reality)\", \"meinamix_meinaV10(Reality)\"]:\n",
        "    for del_word in del_word_list:\n",
        "        model_select=model_select.replace(del_word, \"\" )\n",
        "#@markdown モデルの選択方法\n",
        "\n",
        "#@markdown * ボックスの右の三角を押すと出るリストから選択\n",
        "\n",
        "#@markdown * URLを入力\n",
        "\n",
        "#@markdown * モデル名を入力\n",
        "\n",
        "#@markdown * モデルが保存されているパスを入力\n",
        "\n",
        "#@markdown *  **search** と入力すると全てのディレクトリから候補を探します。\n",
        "\n",
        "## @markdown >モードの切り替え (mode change)\n",
        "#mode_select = \"fp16\" #@param [\"fp32\",\"fp16\"]\n",
        "\n",
        "\n",
        "\n",
        "#hub_select = \"hugface\" #@param [\"hugface\",\"civitai\"]\n",
        "\n",
        "\n",
        "#@markdown >追加機能\n",
        "\n",
        "auto = False  # @param {type:\"boolean\"}\n",
        "# @markdown リポジトリ & モデルファイル を自動で選択します(推奨: ON)\n",
        "\n",
        "\n",
        "\n",
        "if \"step1_finish\" not in locals():\n",
        "    raise NameError(\"\\033[33m先にStep.1の実行をお願いします\\033[0m\")\n",
        "\n",
        "if \"count_number\" not in locals():\n",
        "    count_number=0\n",
        "    input_skip=False\n",
        "\n",
        "if \"model_url\" not in locals():\n",
        "    model_url=\"\"\n",
        "\n",
        "if (\"token_list\" not in locals()) and (\"token_list\" not in globals()):\n",
        "    token_list=[]\n",
        "\n",
        "\n",
        "\n",
        "class version_sort:\n",
        "    def extract_version(self, version_string) -> str:\n",
        "        match = re.search(r'[vV\\-_](0?\\d+(\\.\\d+)*)', version_string)\n",
        "        if match:\n",
        "            return match.group(1)\n",
        "        else:\n",
        "            return \"0\"\n",
        "\n",
        "    def convert_version(self, version: str) -> int:\n",
        "        converted_version = 0\n",
        "        version_parts = version.split('.')\n",
        "        if len(version_parts) > 0:\n",
        "            for i in range(len(version_parts)):\n",
        "                if i == len(version_parts) - 1:\n",
        "                    converted_version += int(version_parts[i])\n",
        "                else:\n",
        "                    converted_version += int(version_parts[i]) * (100 ** (len(version_parts) - i - 1))\n",
        "        return converted_version\n",
        "\n",
        "    def sort(self,mylist) -> list:\n",
        "        version_dict = {}\n",
        "        for item in mylist:\n",
        "            version = self.extract_version(item)\n",
        "            converted_version = self.convert_version(version)\n",
        "            version_dict[item] = converted_version\n",
        "        sorted_list = sorted(version_dict, key=lambda x: version_dict[x], reverse=True)\n",
        "        return sorted_list\n",
        "\n",
        "#version=version_sort()\n",
        "\n",
        "exts =  [\".safetensors\", \".ckpt\"]\n",
        "\n",
        "exclude =  [\"safety_checker/model.safetensors\",\n",
        "            \"unet/diffusion_pytorch_model.safetensors\",\n",
        "            \"vae/diffusion_pytorch_model.safetensors\",\n",
        "            \"text_encoder/model.safetensors\",\n",
        "            \"unet/diffusion_pytorch_model.fp16.safetensors\",\n",
        "            \"text_encoder/model.fp16.safetensors\",\n",
        "            \"vae/diffusion_pytorch_model.fp16.safetensors\",\n",
        "            \"safety_checker/model.fp16.safetensors\",\n",
        "\n",
        "            \"safety_checker/model.ckpt\",\n",
        "            \"unet/diffusion_pytorch_model.ckpt\",\n",
        "            \"vae/diffusion_pytorch_model.ckpt\",\n",
        "            \"text_encoder/model.ckpt\",\n",
        "            \"text_encoder/model.fp16.ckpt\",\n",
        "            \"safety_checker/model.fp16.ckpt\",\n",
        "            \"unet/diffusion_pytorch_model.fp16.ckpt\",\n",
        "            \"vae/diffusion_pytorch_model.fp16.ckpt\",\n",
        "\n",
        "\n",
        "                 ]\n",
        "\n",
        "\n",
        "def input_fomat(self,Value,zero_txt=\"\", return_txt=\"\",print_txt=\"パス\",auto=False):\n",
        "        if zero_txt and return_txt:\n",
        "            special_state=True\n",
        "            num=0\n",
        "        else:\n",
        "            special_state=False\n",
        "            num=1\n",
        "\n",
        "        if type(Value) is dict:\n",
        "            Value_is_dict=True\n",
        "        else:\n",
        "            Value_is_dict=False\n",
        "\n",
        "        if auto:\n",
        "            if Value_is_dict:\n",
        "                max_pair = max(Value.items(), key=lambda x: x[1][1])\n",
        "                return max_pair[1][0]\n",
        "            else:\n",
        "                return Value[0]\n",
        "\n",
        "        if len(Value)>=15:\n",
        "            start_number=\"1\"\n",
        "            if special_state:\n",
        "                start_number=\"0\"\n",
        "                print(f\"\\033[34m0.{zero_txt}\")\n",
        "\n",
        "            if Value_is_dict:\n",
        "                sorted_with_like = sorted(Value.items(), key=lambda x: x[1][1], reverse=True)\n",
        "                for i, (model_id, (model_name, like)) in enumerate(sorted_with_like, 1):\n",
        "                    if i>15:\n",
        "                        break\n",
        "                    print(f\"\\033[34m{i}.{print_txt}: {model_name}, 評価: {like}\")\n",
        "            else:\n",
        "                for i in range(1,16):#15個\n",
        "                    print(f\"\\033[34m{i+1}.{print_txt}: {Value[i]}\\033[0m\")\n",
        "\n",
        "\n",
        "            print(f\"\\033[34m16.上記の{print_txt}以外 (全ての候補を表示します)\\033[0m\\n\")\n",
        "            while True:\n",
        "                choice = input(f\"{print_txt}の選択({start_number}~16): \")\n",
        "                try:\n",
        "                    choice=int(choice)\n",
        "                except ValueError:\n",
        "                    print(\"\\033[33m自然数のみ有効です\\033[34m\")\n",
        "                    continue\n",
        "                if special_state and choice==0:\n",
        "                    print(\"\\033[0m\",end=\"\")\n",
        "                    return return_txt\n",
        "\n",
        "                if choice==16: #other_file\n",
        "                    break\n",
        "                elif 1<=choice<=15:\n",
        "                    if Value_is_dict:\n",
        "                        #choice_path,Like=Value[choice-1]\n",
        "                        choice_path = Value[list(Value.keys())[choice - 1]]\n",
        "                    else:\n",
        "                        choice_path=Value[choice-1]\n",
        "                    print(\"\\033[0m\",end=\"\")\n",
        "                    return choice_path\n",
        "                else:\n",
        "                    print(f\"\\033[33m{num}~16 までの数字の入力をお願いします\\033[34m\")\n",
        "\n",
        "        start_number=\"1\"\n",
        "        if special_state:\n",
        "            start_number=\"0\"\n",
        "            print(f\"\\033[34m0.{zero_txt}\")\n",
        "        for i, file_name in enumerate(Value, 1):\n",
        "            print(f\"\\033[34m{i}.{print_txt}: {file_name}\")\n",
        "        while True:\n",
        "            choice = input(f\"使用する{print_txt}の選択({start_number}~{len(Value)}): \")\n",
        "            try:\n",
        "                choice=int(choice)\n",
        "            except ValueError:\n",
        "                print(\"\\033[33m自然数のみ有効です\\033[34m\")\n",
        "                continue\n",
        "            if special_state and choice==0:\n",
        "                print(\"\\033[0m\",end=\"\")\n",
        "                return return_txt\n",
        "            if 1<=choice<=len(Value):\n",
        "                if Value_is_dict:\n",
        "                    #choice_path=Value[choice-1]\n",
        "                    choice_path = Value[list(Value.keys())[choice - 1]]\n",
        "                else:\n",
        "                    choice_path=Value[choice-1]\n",
        "                print(\"\\033[0m\",end=\"\")\n",
        "                return choice_path\n",
        "            else:\n",
        "                print(f\"\\033[33m{num}~{len(Value)} までの数字の入力をお願いします\\033[34m\")\n",
        "        print(\"\\033[0m\",end=\"\")\n",
        "        return choice_path\n",
        "\n",
        "\n",
        "\n",
        "class Huggingface(version_sort):\n",
        "    def __init__(self,model_select,device,hub_select,auto):\n",
        "        device=device\n",
        "        self.model_id, self.model_name, self.vae_name, self.model_file=\"\",\"\",\"\",\"\"\n",
        "        self.input_url=False\n",
        "        self.diffuser_model=False\n",
        "        self.exts=\"\"\n",
        "        self.auto=auto\n",
        "        self.model_select=model_select\n",
        "        self.hub=hub_select\n",
        "        self.file_path_dict={}\n",
        "        self.special_file=\"\"\n",
        "\n",
        "\n",
        "        self.Error_M1= (\n",
        "                   '''URLを読み込めませんでした。この機能がサポートしている形式は次の通りです。\n",
        "                      形式:\"https://huggingface.co/<repo_name>/<model_name>/blob/main/<path_to_file>\"\n",
        "                      例1: \"https://huggingface.co/gsdf/Counterfeit-V3.0/blob/main/Counterfeit-V3.0.safetensors\"\n",
        "                      例2: \"https://huggingface.co/runwayml/stable-diffusion-v1-5/blob/main/v1-5-pruned.ckpt\"\n",
        "                      補足:huggingfaceにあれば、diffusersのタグがついていないモデルでも、ほとんどの場合使用できます。\n",
        "                    ''')\n",
        "        self.Error_M2= (\n",
        "            '''hugface_pathを読み込めませんでした。この機能がサポートしている形式は次の通りです。\n",
        "            形式: <repo_name>/<model_name>\"\n",
        "            例1: \"Linaqruf/anything-v3.0\"\n",
        "            例2: \"stabilityai/stable-diffusion-2-1\"\n",
        "\n",
        "            (下のモデル名をコピー&ペースト、または、model_selectの右側の三角を押すと出てくるドロップダウンからお選びください。)\n",
        "\n",
        "                      \"stable diffusion-v2.1\"\n",
        "                      \"waifu diffusion-v1.4\"\n",
        "                      \"Anything-v3.0\"\n",
        "                      \"anything-midjourney-v-4-1\"\n",
        "                      \"Anything-v4.5\"\n",
        "                      \"AB4.5_AC0.2\"\n",
        "                      \"basil_mix\"\n",
        "                      \"Waifu-Diffusers\"\n",
        "                      \"Double-Exposure-Diffusion\"\n",
        "                      \"openjourney-v4\"\n",
        "                      \"ACertainThing\"\n",
        "                      \"Counterfeit-V2.0\"\n",
        "                      \"Counterfeit-V2.5\"\n",
        "                      \"7th_Layer\"\n",
        "                      \"EimisAnimeDiffusion_1.0v\"\n",
        "                      \"Riga_Collection\"\n",
        "                      \"anime-kawai-diffusion\"\n",
        "                      \"Realistic_Vision_V2.0\"\n",
        "                      \"meinamix_meinaV10\"\n",
        "                      \"loliDiffusion\"\n",
        "                      ''')\n",
        "        self.Error_M3=(\n",
        "                  '''指定されたパスを認識できませんでした。次をお試しください\n",
        "                     ・ファイルのパスが存在するかの確認\n",
        "                     ・空白が混入していないかの確認(pathの最後に半角空白が意図せずつくことがあります)\n",
        "                     ・\"\\\"や、拡張子以外の\".\"などの特殊記号が含まれているかの確認(認識されない可能性があります)\n",
        "                  ''')\n",
        "\n",
        "        self.special_exc=[\"text_encoder/\",\n",
        "                          \"unet/\",\n",
        "                          \"vae/\",]\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "        self.model_dict = {\n",
        "            \"stable diffusion-v2.1\" : \"stabilityai/stable-diffusion-2-1\",\n",
        "            \"waifu diffusion-v1.4\": \"hakurei/waifu-diffusion\",\n",
        "            \"Anything-v3.0\": \"Linaqruf/anything-v3.0\",\n",
        "            \"anything-midjourney-v-4-1\": \"Joeythemonster/anything-midjourney-v-4-1\",\n",
        "            \"Anything-v4.5\": \"shibal1/anything-v4.5-clone\",\n",
        "            \"AB4.5_AC0.2\": \"aioe/AB4.5_AC0.2\",\n",
        "            \"basil_mix\": \"nuigurumi/basil_mix\",\n",
        "            \"Waifu-Diffusers\": \"Nilaier/Waifu-Diffusers\",\n",
        "            \"Double-Exposure-Diffusion\": \"joachimsallstrom/Double-Exposure-Diffusion\",\n",
        "            \"openjourney-v4\": \"prompthero/openjourney-v4\",\n",
        "            \"ACertainThing\": \"JosephusCheung/ACertainThing\",\n",
        "            \"Counterfeit-V2.0\": \"gsdf/Counterfeit-V2.0\",\n",
        "            \"Counterfeit-V2.5\": \"gsdf/Counterfeit-V2.5\",\n",
        "            \"chilled_remix\":\"chilled_remix\",\n",
        "            \"chilled_reversemix\":\"chilled_reversemix\",\n",
        "            \"7th_Layer\": \"syaimu/7th_test\",\n",
        "            \"EimisAnimeDiffusion_1.0v\": \"eimiss/EimisAnimeDiffusion_1.0v\",\n",
        "            \"JWST-Deep-Space-diffusion\" : \"dallinmackay/JWST-Deep-Space-diffusion\",\n",
        "            \"Riga_Collection\": \"natsusakiyomi/Riga_Collection\",\n",
        "            \"sd-db-epic-space-machine\" : \"rabidgremlin/sd-db-epic-space-machine\",\n",
        "            \"spacemidj\" : \"Falah/spacemidj\",\n",
        "            \"anime-kawai-diffusion\": \"Ojimi/anime-kawai-diffusion\",\n",
        "            \"Realistic_Vision_V2.0\": \"SG161222/Realistic_Vision_V2.0\",\n",
        "            \"nasa-space-v2\" : \"sd-dreambooth-library/nasa-space-v2-768\",\n",
        "            \"meinamix_meinaV10\": \"namvuong96/civit_meinamix_meinaV10\",\n",
        "            \"loliDiffusion\": \"JosefJilek/loliDiffusion\",\n",
        "            }\n",
        "\n",
        "\n",
        "        self.EN='''埋め込み: 有効\\n有効化の鍵: <EasyNegative>,<bad-hands>\\n\n",
        "                   ※埋め込みを使用する場合、「有効化の鍵」を＜＞ごとコピー＆ペーストしてください'''\n",
        "\n",
        "\n",
        "    def model_safe_check(self,model_list) ->str:\n",
        "        if len(model_list)>1:\n",
        "           for check_model in model_list:\n",
        "                match = bool(re.search(r\"(?i)[-＿]sfw\", check_model))\n",
        "                if match:\n",
        "                    return check_model\n",
        "        return model_list[0]\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    def diffusers_model_check(self,checked_model: str) -> bool:\n",
        "        index_url=f\"https://huggingface.co/{checked_model}/blob/main/model_index.json\"\n",
        "        return check_url(index_url)\n",
        "\n",
        "    def data_get(self,path) -> list:\n",
        "        url = f\"https://huggingface.co/api/models/{path}\"\n",
        "        data = requests.get(url).json()\n",
        "        file_value_list = []\n",
        "        df_model_bool=False\n",
        "        #fix error': 'Repo model <repo_id>/<model> is gated. You must be authenticated to access it.\n",
        "        try:\n",
        "            siblings=data[\"siblings\"]\n",
        "        except KeyError:\n",
        "            return []\n",
        "\n",
        "        for item in siblings:\n",
        "            data[\"siblings\"]\n",
        "            file_path=item[\"rfilename\"]\n",
        "            #model_index.json outside the root directory is not recognized\n",
        "            if file_path==\"model_index.json\":\n",
        "                df_model_bool=True\n",
        "            elif (any(file_path.endswith(ext) for ext in exts) and\n",
        "                not any(file_path.endswith(ex) for ex in exclude)):\n",
        "                file_value_list.append(file_path)\n",
        "        #↓{df_model,file_value_list}\n",
        "        self.file_path_dict.update({path:(df_model_bool,file_value_list)})\n",
        "        return file_value_list\n",
        "\n",
        "    def model_name_search(self,model_name,auto_set):\n",
        "        #\"auto(bool)\" loads the model with the most likes in hugface\n",
        "        url = f\"https://huggingface.co/api/models\"#?search={model_name}\"\n",
        "        params={\"search\":model_name,\"sort\":\"likes\",\"direction\":-1}#\"downloads\",}\n",
        "        data = requests.get(url,params=params).json()\n",
        "        with_like=[]\n",
        "        choice_path=\"\"\n",
        "        repo_model_list = []\n",
        "        if data:\n",
        "            for item in data:\n",
        "                model_id,like,private_value,tag_value = item[\"modelId\"],item[\"likes\"],item[\"private\"],item[\"tags\"]\n",
        "                if (\"Flax\" not in tag_value and\n",
        "                    \"TPU\" not in tag_value and\n",
        "                    \"audio-to-audio\" not in tag_value and\n",
        "                    (not private_value)):\n",
        "                    File_list=self.data_get(model_id)\n",
        "                    if File_list:\n",
        "                        repo_model_list.append(model_id)\n",
        "                        with_like.append([model_id,like])\n",
        "        else:\n",
        "            print(\"hugfaceでは適合するモデルがみつかりませんでした\")\n",
        "            return \"_hf_no_model\"\n",
        "\n",
        "        if not auto_set:\n",
        "            print(\"\\033[34m以下のモデルパスが見つかりました。\")\n",
        "            if len(with_like)>=15:\n",
        "                print(\"0.civiaiを検索\")\n",
        "                for (i,(model_name,like)) in enumerate(with_like,1):\n",
        "                    if i>15:\n",
        "                        break\n",
        "                    #model_name, like=sorted_with_like[i]\n",
        "                    print(f\"{i}.モデルパス: {model_name}, 評価: {like}\")\n",
        "                print(\"16.上記以外\")\n",
        "                while True:\n",
        "                    try:\n",
        "                        choice = int(input(\"使用するモデルパスの選択: \"))\n",
        "                    except ValueError:\n",
        "                        print(\"\\033[33m自然数のみ有効です\\033[34m\")\n",
        "                        continue\n",
        "                    if choice==0:\n",
        "                        return \"_hf_no_model\"\n",
        "                    elif choice==16:\n",
        "                        break\n",
        "                    elif 1<=choice<=15:\n",
        "                        choice_path=repo_model_list[choice-1]\n",
        "                        return choice_path\n",
        "                    else:\n",
        "                        print(f\"1~{len(repo_model_list)} までの数字の入力をお願いします\")\n",
        "\n",
        "            print(\"0.civiaiを検索\")\n",
        "            for (s,(model_name,like)) in enumerate(with_like,1):\n",
        "                print(f\"{s}.モデルパス: {model_name}, 評価: {like}\")\n",
        "            while True:\n",
        "                try:\n",
        "                    choice = int(input(\"使用するモデルパスの選択: \"))\n",
        "                except ValueError:\n",
        "                    print(\"\\033[33m自然数のみ有効です\\033[34m\")\n",
        "                    continue\n",
        "                if choice==0:\n",
        "                    return \"_hf_no_model\"\n",
        "                elif 1<=choice<=len(repo_model_list):\n",
        "                    choice_path=repo_model_list[choice-1]\n",
        "                    return choice_path\n",
        "                else:\n",
        "                    print(f\"\\033[33m1~{len(repo_model_list)} までの数字の入力をお願いします\\033[34m\")\n",
        "        else:\n",
        "            choice_path = with_like[0][0]\n",
        "\n",
        "            return choice_path\n",
        "\n",
        "\n",
        "    def file_name_set_sub(self,model_select,file_value):\n",
        "        global old_num\n",
        "        if \"old_num\" not in globals():\n",
        "            old_num= None\n",
        "        elif key_check(self.model_select) and old_num is not None and old_num!=\"_DFmodel\":\n",
        "            print(f\"※前回の番号: {old_num}\")\n",
        "\n",
        "        if (not file_value) and (not self.diffuser_model):\n",
        "            #return \"_hf_no_model\"\n",
        "            raise ValueError(\"指定されたリポジトリから使用可能なファイルが見つかりませんでした\")\n",
        "        elif not file_value:\n",
        "            print(\"\\033[34m通常のDiffusersモデルのみが見つかりました\")\n",
        "            while True:\n",
        "                result=input(\"使用しますか?[y/n]: \")\n",
        "                if result in [\"y\",\"Y\"]:\n",
        "                    return \"_DFmodel\"\n",
        "                elif result in [\"n\",\"N\"]:\n",
        "                    sec_result=input(\"civiaiの検索を実行しますか？[y/n]: \")\n",
        "                    if sec_result in [\"y\",\"Y\"]:\n",
        "                        return \"_hf_no_model\"\n",
        "                    elif sec_result in [\"n\",\"N\"]:\n",
        "                        raise ValueError(\"該当するモデルが見つからなかった為、処理を停止しました\")\n",
        "                else:\n",
        "                    print(\"[y,n]のみの入力をお願いします\")\n",
        "\n",
        "\n",
        "        if len(file_value)>=15:\n",
        "            start_number=\"1\"\n",
        "            if self.diffuser_model:\n",
        "                start_number=\"0\"\n",
        "                print(\"\\033[34m0.通常のDiffusersモデルを使用\")\n",
        "            for i in range(15):\n",
        "                print(f\"\\033[34m{i+1}.ファイル名: {file_value[i]}\\033[0m\")\n",
        "            print(\"\\033[34m16.上記のファイル以外 (全ての候補を表示します)\\033[0m\\n\")\n",
        "            while True:\n",
        "                choice = input(f\"使用するファイルの選択({start_number}~16): \")\n",
        "                try:\n",
        "                    choice=int(choice)\n",
        "                except ValueError:\n",
        "                    print(\"\\033[33m自然数のみ有効です\\033[34m\")\n",
        "                    continue\n",
        "                if self.diffuser_model and choice==0:\n",
        "                    old_num=None\n",
        "                    self.input_url=False\n",
        "                    print(\"\\033[0m\",end=\"\")\n",
        "                    return \"_DFmodel\"\n",
        "\n",
        "                elif choice==16: #other_file\n",
        "                    break\n",
        "                elif 0<=choice<=len(file_value):\n",
        "                    self.input_url=True\n",
        "                    old_num=choice\n",
        "                    choice_path=file_value[choice-1]\n",
        "                    print(\"\\033[0m\",end=\"\")\n",
        "                    return choice_path\n",
        "                else:\n",
        "                    print(f\"\\033[33m1~16 までの数字の入力をお願いします\\033[34m\")\n",
        "            print(\"\\033[0m\",end=\"\")\n",
        "            print(\"\\n\\n\")\n",
        "\n",
        "        start_number=\"1\"\n",
        "        if self.diffuser_model:\n",
        "            start_number=\"0\"\n",
        "            print(\"\\033[34m0.通常のDiffusersモデルを使用\\033[0m\")\n",
        "        for i, file_name in enumerate(file_value, 1):\n",
        "            print(f\"\\033[34m{i}.ファイル名: {file_name}\")\n",
        "        while True:\n",
        "            choice = input(f\"使用するファイルの選択({start_number}~{len(file_value)}): \")\n",
        "            try:\n",
        "                choice=int(choice)\n",
        "            except ValueError:\n",
        "                print(\"\\033[33m自然数のみ有効です\\033[34m\")\n",
        "                continue\n",
        "            if self.diffuser_model and choice==0:\n",
        "                self.input_url=False\n",
        "                old_num=None\n",
        "                print(\"\\033[0m\",end=\"\")\n",
        "                return \"_DFmodel\"\n",
        "            if 1<=choice<=len(file_value):\n",
        "                self.input_url=True\n",
        "                old_num=choice\n",
        "                choice_path=file_value[choice-1]\n",
        "                return choice_path\n",
        "            else:\n",
        "                print(f\"\\033[33m1~{len(file_value)} までの数字の入力をお願いします\\033[34m\")\n",
        "        print(\"\\033[0m\",end=\"\")\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    def file_name_set(self,model_select,auto,model_type=\"Checkpoint\"):\n",
        "        if self.diffusers_model_check(model_select) and model_type==\"Checkpoint\":\n",
        "            self.diffuser_model=True\n",
        "        url = f\"https://huggingface.co/api/models/{model_select}\"\n",
        "        response = requests.get(url)\n",
        "        data = response.json()\n",
        "        choice_path=\"\"\n",
        "        file_value = []\n",
        "        siblings = data[\"siblings\"]\n",
        "        if data:\n",
        "            for item in siblings:\n",
        "                fi_path=item[\"rfilename\"]\n",
        "                if (any(fi_path.endswith(ext) for ext in exts) and\n",
        "                    not any(fi_path.endswith(ex) for ex in exclude)):\n",
        "                    file_value.append(fi_path)\n",
        "        else:\n",
        "            raise ValueError(\"使用可能なファイルが見つかりませんでした。\\n名前の確認をお試しください\")\n",
        "        if file_value:\n",
        "            file_value=self.sort(file_value)\n",
        "            if not auto:\n",
        "                print(\"以下のモデルファイルが見つかりました。\")\n",
        "                choice_path=self.file_name_set_sub(model_select,file_value)\n",
        "            else:\n",
        "                if self.diffuser_model:\n",
        "                    self.input_url=False\n",
        "                else:\n",
        "                    self.input_url=True\n",
        "                    choice_path=self.model_safe_check(file_value)\n",
        "\n",
        "\n",
        "        elif self.diffuser_model:\n",
        "            print(\"\\033[32m通常のDifusersモデルのみが見つかりました\")\n",
        "            return \"_DFmodel\"\n",
        "        else:\n",
        "            raise FileNotFoundError(\"指定されたリポジトリに使用可能なファイルが見つかりません\")\n",
        "        if model_type!=\"Checkpoint\" and model_type!=\"_DFmodel\":\n",
        "            self.input_url=False\n",
        "            choice_path=hf_hub_download(repo_id=model_select, filename=choice_path)\n",
        "        return choice_path\n",
        "\n",
        "\n",
        "class Civitai:\n",
        "    def public_civiai(self,model_select,hub,auto,model_type):\n",
        "        url,filename=self.civitai_download(model_select,auto,model_type)\n",
        "        filename=filename.replace(\" \", \"_\")\n",
        "        Civiai_dir=\"/content/model/other_model\"\n",
        "        Civiai_save_path=os.path.join(Civiai_dir,filename)\n",
        "        os.makedirs(Civiai_dir,exist_ok=True)\n",
        "        self.use_civitai(url,Civiai_save_path)\n",
        "        return Civiai_save_path\n",
        "\n",
        "    def use_civitai(self,url,savename):\n",
        "        if not url in download_list:\n",
        "            os.chdir(\"/content/model/other_model\")\n",
        "            os.environ[\"path\"]=url\n",
        "            os.environ[\"save\"]=savename\n",
        "            !wget -q --show-progress --content-disposition $path  -O$save\n",
        "            download_list.append(url)\n",
        "            os.chdir(\"/content\")\n",
        "        elif url in download_list:\n",
        "            print(\"モデルはダウンロード済です\")\n",
        "\n",
        "        else:\n",
        "            if not url in download_list:\n",
        "                raise HTTPError(\"URLが無効です\")\n",
        "\n",
        "    def civitai_download(self,query,auto_set,model_type):\n",
        "        state={}\n",
        "        durl_list=[]\n",
        "        files_value=[]\n",
        "        durl=\"\"\n",
        "        filename=\"\"\n",
        "        like=0\n",
        "        choice_path=\"\"\n",
        "        choice_file=\"\"\n",
        "        params = {\"query\": query,\"types\":model_type,\"sort\":\"Most Downloaded\"}\n",
        "        response = requests.get(\"https://civitai.com/api/v1/models\", params=params)\n",
        "        if response.status_code == 200:\n",
        "            data = response.json()\n",
        "            items = data[\"items\"]\n",
        "            if items:\n",
        "                for item in items:\n",
        "                    checked_files=item[\"modelVersions\"]\n",
        "                    for check in checked_files:\n",
        "                        if \"files\" in check:\n",
        "                            files_value=check[\"files\"]\n",
        "                        repo_name=item[\"name\"]\n",
        "                        stat=item[\"stats\"]\n",
        "                        likes=stat[\"favoriteCount\"]\n",
        "                        for isa in item[\"modelVersions\"]:\n",
        "                            isa_files=isa[\"files\"]\n",
        "                            if isa_files:#important\n",
        "                                for ss in isa_files:\n",
        "                                    if \"downloadUrl\" in ss and \"name\" in ss:\n",
        "                                        durl=ss[\"downloadUrl\"]\n",
        "                                        filename=ss[\"name\"]\n",
        "                                        break\n",
        "                                    else:\n",
        "                                        durl,filename=\"\",\"\"\n",
        "                        if not durl==\"\":\n",
        "                            durl_list.append([durl,filename])\n",
        "                            state.update({len(state) + 1: (repo_name,likes)})\n",
        "\n",
        "                if state:\n",
        "                    if not auto_set:\n",
        "                        sorted_with_like = sorted(state.items(), key=lambda x: x[1][1], reverse=True)\n",
        "                        #if not len(state)==1:\n",
        "                        print(\"以下のモデルパスが見つかりました\")\n",
        "                        if len(state)>=15:\n",
        "                            for i in range(15):\n",
        "                                model_name,like=state[i]\n",
        "                                print(f\"\\033[34m{i+1}.モデルパス: {model_name}, 評価: {like}\")\n",
        "\n",
        "                            print(\"16.上記以外\")\n",
        "                            while True:\n",
        "                                    choice = input(\"使用するモデルパスの選択: \")\n",
        "                                    try:\n",
        "                                        choice=int(choice)\n",
        "                                    except ValueError:\n",
        "                                        print(\"\\033[33m自然数のみ有効です\\033[34m\")\n",
        "                                        continue\n",
        "                                    if choice==16:\n",
        "                                        break\n",
        "                                    if 1<=choice<=15:\n",
        "                                        choice_path,choice_file=durl_list[choice-1]\n",
        "                                        return choice_path,choice_file\n",
        "                                    else:\n",
        "                                        print(f\"\\033[33m1~{len(durl_list)} までの数字の入力をお願いします\\033[34m\")\n",
        "\n",
        "                        for i, (model_name, like) in enumerate(sorted_with_like, 1):\n",
        "                            print(f\"\\033[34m{i}.モデルパス: {model_name}, 評価: {like}\")\n",
        "                        while True:\n",
        "                                try:\n",
        "                                    choice=int(input(\"使用するモデルパスの選択: \"))\n",
        "                                except ValueError:\n",
        "                                    print(\"\\033[33m自然数のみ有効です\\033[34m\")\n",
        "                                    continue\n",
        "                                if 1<=choice<=len(durl_list):\n",
        "                                    choice_path,choice_file=durl_list[choice-1]\n",
        "                                    return choice_path,choice_file\n",
        "                                else:\n",
        "                                    print(f\"\\033[33m1~{len(durl_list)} までの数字の入力をお願いします\\033[34m\")\n",
        "                        #else:\n",
        "                        #    self.input_url=True\n",
        "                        #    choice_path,choice_file=durl_list[0]\n",
        "                    else:\n",
        "\n",
        "                        max_likes = max(state, key=lambda x: state[x][1])\n",
        "                        choice_path,choice_file = durl_list[max_likes]\n",
        "\n",
        "                else:\n",
        "                    raise ValueError(\"モデル情報の辞書が空です\")\n",
        "\n",
        "            else:\n",
        "                raise ValueError(\"習得できたモデル情報が空です\")\n",
        "        else:\n",
        "           raise HTTPError(\"モデル情報の習得に失敗しました\")\n",
        "        return choice_path,choice_file\n",
        "\n",
        "\n",
        "\n",
        "class pipeline_setup(Huggingface,Civitai):\n",
        "    def __init__(self,model_select,device,hub_select,auto):\n",
        "        Huggingface.__init__(self,model_select,device,hub_select,auto)\n",
        "        hub_select=hub_select\n",
        "\n",
        "    def File_search(self):\n",
        "        search_path=\"\"\n",
        "        paths = []\n",
        "        for root, dirs, files in os.walk(\"/\"):\n",
        "            for file in files:\n",
        "                if any(file.endswith(ext) for ext in exts):\n",
        "                    path = os.path.join(root, file)\n",
        "                    if path not in exclude:\n",
        "                        if not path.startswith(\"/root/.cache\"):\n",
        "                            paths.append(path)\n",
        "        num_path=len(paths)\n",
        "        if num_path == 0:\n",
        "            raise FileNotFoundError(\"\\033[33mモデルファイルが見つかりませんでした\\033[0m\")\n",
        "        else:\n",
        "            print(f\"モデルファイルの候補が{num_path}個見つかりました。\")\n",
        "        for s, path in enumerate(paths, 1):\n",
        "            print(f\"{s}: {path}\")\n",
        "        num = int(input(f\"番号を入力してください(1〜{num_path}): \"))\n",
        "        if 1 <= num <= len(paths):\n",
        "            search_path=(paths[num-1])\n",
        "            print(f\"選択されたモデルファイル: {search_path}\\n\")\n",
        "            print('設定を変更するが、上記のモデルファイルをそのまま使う場合は、\"入力を飛ばす\"の適用を推奨します\\n')\n",
        "        else:\n",
        "            raise TypeError(f\"\\033[33m無効な文字です。有効な数字: 1〜{len(paths)}\\033[0m\")\n",
        "        return search_path\n",
        "\n",
        "\n",
        "    def model_set(self,model_select,model_type=\"Checkpoint\",hub_select=\"hugface\",auto=False) -> str:\n",
        "        self.auto=auto\n",
        "        if not model_type  in [\"Checkpoint\", \"TextualInversion\", \"LORA\", \"Hypernetwork\", \"AestheticGradient\", \"Controlnet\", \"Poses\"]:\n",
        "            raise TypeError('Wrong argument. Valid values are \"Checkpoint\", \"TextualInversion\", \"LORA\", \"Hypernetwork\", \"AestheticGradient\", \"Controlnet\", \"Poses\"')\n",
        "        if not hub_select in [\"hugface\",\"civitai\"]:\n",
        "            raise TypeError('Wrong argument. Valid values are \"hugface\" or \"civitai\"')\n",
        "        self.input_url=False\n",
        "        model_path,file_path=\"\",\"\"\n",
        "        #辞書にあるリポジトリを優先的に使う処理(テスト)\n",
        "        if model_select in self.model_dict and hub_select==\"hugface\":\n",
        "            model_path_to_check=self.model_dict[model_select]\n",
        "            if check_url(f\"https://huggingface.co/{model_path_to_check}\"):\n",
        "                model_select=model_path_to_check\n",
        "\n",
        "\n",
        "        if model_select==\"search\":\n",
        "            model_path=self.File_search()\n",
        "            self.input_url=True\n",
        "\n",
        "        elif model_select.startswith(\"https://huggingface.co/\"):\n",
        "            self.input_url=True\n",
        "            if not check_url(model_select):\n",
        "                raise ValueError(self.Error_M1)\n",
        "            else:\n",
        "                model_path=model_select\n",
        "\n",
        "        elif model_select.startswith(\"https://civitai.com/\"):\n",
        "            model_path=self.public_civiai(model_select,hub_select,auto,model_type)\n",
        "\n",
        "\n",
        "        elif os.path.isfile(model_select):\n",
        "            self.input_url=True\n",
        "            model_path=model_select\n",
        "\n",
        "        elif \"/\" in model_select and model_select.count(\"/\") == 1:\n",
        "            if auto and self.diffusers_model_check(model_select):\n",
        "                self.input_url=False\n",
        "                return model_select\n",
        "            else:\n",
        "                file_path=self.file_name_set(model_select,auto,model_type)\n",
        "                if file_path==\"_hf_no_model\":\n",
        "                    raise ValueError(\"モデルが見つかりませんでした\")\n",
        "\n",
        "                elif file_path==\"_DFmodel\":\n",
        "\n",
        "                    self.input_url=False\n",
        "                    return model_select\n",
        "                else:\n",
        "                    self.input_url=True\n",
        "                    model_path=f\"https://huggingface.co/{model_select}/blob/main/{file_path}\"\n",
        "\n",
        "\n",
        "        else:\n",
        "            model_path=\"\"\n",
        "            model_name_path=self.model_name_search(model_select,auto)\n",
        "            if not model_name_path==\"_hf_no_model\":\n",
        "                model_path=self.file_name_set(model_name_path,auto,model_type)\n",
        "            else:\n",
        "                ci_model_path=self.public_civiai(model_select,self.hub,self.auto,model_type)\n",
        "                self.input_url=True\n",
        "                return ci_model_path\n",
        "\n",
        "            if model_path==\"_DFmodel\":\n",
        "                self.input_url=False\n",
        "                return model_name_path\n",
        "            elif self.input_url:\n",
        "                model_path=f\"https://huggingface.co/{model_name_path}/blob/main/{model_path}\"\n",
        "            else:\n",
        "                raise ValueError(\"モデルが見つかりません\")\n",
        "        return model_path\n",
        "\n",
        "\n",
        "    def pipe_create(self,model_path,device):\n",
        "        if self.input_url:\n",
        "            base_pipe = StableDiffusionPipeline.from_single_file(\n",
        "                model_path,torch_dtype=torch.float16\n",
        "                ).to(device)\n",
        "            print(f\"\\033[32mモデルPath:{model_path}\")\n",
        "        else:\n",
        "            base_pipe = StableDiffusionPipeline.from_pretrained(\n",
        "                model_path,torch_dtype=torch.float16\n",
        "                ).to(device)\n",
        "            print(f\"\\033[32mモデルID: {model_path}\")\n",
        "        return base_pipe\n",
        "\n",
        "hub_select=\"hugface\"#\"civitai\"\n",
        "pipe_set=pipeline_setup(model_select,device,hub_select,auto)\n",
        "\n",
        "\n",
        "model_path=pipe_set.model_set(model_select,hub_select=hub_select,auto=auto)\n",
        "base_pipe=pipe_set.pipe_create(model_path,device)\n",
        "\n",
        "gc.collect()\n",
        "\n",
        "print(\"\\033[32mモデルのセットアップが終了しました\\033[0m\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gLCtR5TwuUvY"
      },
      "outputs": [],
      "source": [
        "#@title  #Step.3 パイプラインの設定 (Pipeline Setup){display-mode: \"form\"}\n",
        "\n",
        "# @markdown >パイプラインの種類\n",
        "\n",
        "pipe_type = \"txt2img\" # @param [\"txt2img\", \"img2img\", \"Inpaint\", \"Depth2Img\", \"txt2video\", \"safe\"] {allow-input: true}\n",
        "\n",
        "\n",
        "#@markdown >スケジューラーを変更\n",
        "\n",
        "Scheduler_select = \"DPM\" # @param [\"DPM\",\"DDPM\",\"DDIM\",\"DEISM\",\"DPM_S\",\"EulerA\",\"Euler\",\"HeunD\",\"K_DPM2D\",\"K_DPM2AD\",\"LMSD\",\"PNDM\",\"UniPCM\",\"CustomEulerA\",\"CustomEuler\"] {allow-input: true}\n",
        "if not Scheduler_select:\n",
        "    raise TypeError(\"Scheduler_select has not been entered\")\n",
        "#@markdown >vaeを選択 (vaeを交換したい時のみ入力)\n",
        "vae_select = \"\" # @param [\"waifu-diffusion\", \"Counterfeit-V2.5\", \"anything-v3.0\"] {allow-input: true}\n",
        "\n",
        "# @markdown >フィルターを調整\n",
        "\n",
        "\n",
        "# @markdown **注意事項 : 変更する時は注意して下さい**\n",
        "\n",
        "Filter_off = False  # @param {type:\"boolean\"}\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "if vae_select:\n",
        "    vae_change=True\n",
        "    print(\"vaeを変更します\")\n",
        "else:\n",
        "    vae_change=False\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "if pipe_type==\"safe\":\n",
        "    if Scheduler_select ==\"CustomEulerA\" or Scheduler_select == \"CustomEuler\":\n",
        "        Scheduler_select=\"DDIM\"\n",
        "        print('safe_pipelineでは、ソナー付きの\"EulerA\" or \"Euler\"を使用すると絵が崩壊するバグが発生しているので、ddimに設定しました。')\n",
        "    elif pipe_type==\"safe\" and Scheduler_select!=(\"DDIM\" or \"PNDM\"or\"LMSD\"):\n",
        "        print('safe_pipelineの場合、Schedulerは\"ddim\"or\"pndm\"のみ使用可能です。(Schedulerをddimに設定しました)')\n",
        "        Scheduler_select=\"DDIM\"\n",
        "\n",
        "\n",
        "def get_call_method(class_name:str,method_name:str='__call__') ->list:\n",
        "    call_method = getattr(getattr(diffusers, class_name),method_name)\n",
        "    parameters = inspect.signature(call_method).parameters\n",
        "    arg_names = []\n",
        "    for param in parameters.values():\n",
        "        arg_names.append(param.name)\n",
        "    return arg_names\n",
        "\n",
        "def pipe_class_type(class_name):\n",
        "    call_method=get_call_method(class_name,method_name='__call__')\n",
        "    if \"image\" in call_method:\n",
        "        pipeline_type=\"_img2img\"\n",
        "    elif \"video_length\" in call_method:\n",
        "        pipeline_type=\"_txt2video\"\n",
        "    else:\n",
        "        pipeline_type=\"_txt2img\"\n",
        "    return pipeline_type\n",
        "\n",
        "\n",
        "def import_on_str(desired_function_or_class,module_name=\"\",as_arg=\"\"):\n",
        "    if as_arg:\n",
        "        import_name=as_arg\n",
        "    else:\n",
        "        import_name=desired_function_or_class\n",
        "    if not module_name:\n",
        "        globals()[import_name] = __import__(desired_function_or_class)\n",
        "    else:\n",
        "        globals()[import_name] = getattr(__import__(module_name), desired_function_or_class)\n",
        "\n",
        "def max_temper(text, st_list):\n",
        "    similarity_dict = {}\n",
        "    for string in st_list:\n",
        "        similarity = difflib.SequenceMatcher(None, text, string).ratio()\n",
        "        similarity_dict[string] = similarity\n",
        "    most_similar_string = max(similarity_dict, key=similarity_dict.get)\n",
        "\n",
        "    return most_similar_string\n",
        "\n",
        "def sort_list_obj(list_obj,need_txt):\n",
        "    sorted_list=[]\n",
        "    for module_obj in list_obj:\n",
        "        if need_txt in module_obj:\n",
        "            sorted_list.append(module_obj)\n",
        "    return sorted_list\n",
        "\n",
        "def make_Error_message(output_txt,list_obj,need_txt=\"\"):\n",
        "    if need_txt:\n",
        "        _diffusers_module=sort_list_obj(list_obj,need_txt)\n",
        "    else:\n",
        "        _diffusers_module=(dir(diffusers))\n",
        "    pretxt=max_temper(output_txt,_diffusers_module)\n",
        "    return pretxt\n",
        "\n",
        "\n",
        "def pipe_type_set(pipe_type):\n",
        "    #\"StableDiffusionPipeline\"\n",
        "    pipe_class_dict={\n",
        "            \"txt2img\":\"StableDiffusionPipeline\",\n",
        "            \"img2img\":\"StableDiffusionImg2ImgPipeline\",\n",
        "            \"txt2video\":\"TextToVideoZeroPipeline\",\n",
        "            \"Inpaint\":\"StableDiffusionInpaintPipeline\",\n",
        "            \"Depth2Img\":\"StableDiffusionDepth2ImgPipeline\",\n",
        "            \"safe\":\"StableDiffusionPipelineSafe\",\n",
        "            }\n",
        "    if pipe_type in pipe_class_dict:\n",
        "        pipe_type=pipe_class_dict[pipe_type]\n",
        "    try:\n",
        "        import_on_str(pipe_type,module_name=\"diffusers\")\n",
        "    except AttributeError:\n",
        "        #diffusers_module_sorted=sort_list_obj(dir(diffusers),\"pipeline\")\n",
        "        #pretxt=max_temper(pipe_type,diffusers_module_sorted)\n",
        "        pretxt=make_Error_message(pipe_type,dir(diffusers),need_txt=\"pipeline\")\n",
        "        raise AttributeError(f\"'{pipe_type}' not found. Maybe '{pretxt}' ?\")\n",
        "    return pipe_type\n",
        "\n",
        "pipe_type=pipe_type_set(pipe_type)\n",
        "pipeline_class_type=pipe_class_type(pipe_type)\n",
        "\n",
        "\n",
        "if key_check(pipe_type) or \"loaded_Lora_list\" not in globals():\n",
        "    loaded_Lora_list=[]\n",
        "    loaded_textual_list=[]\n",
        "\n",
        "\n",
        "word_format=f\"パイプラインの種類: {pipe_type}\\n\"\n",
        "\n",
        "\n",
        "def result():\n",
        "    if Filter_off == False:\n",
        "        filter_level = \"フィルターの強度:通常\"\n",
        "    else:\n",
        "        filter_level = \"\\033[33mフィルターの強度:無効\\033[0m\"\n",
        "    return filter_level\n",
        "\n",
        "#laion/CLIP-ViT-H-14-laion2B-s32B-b79K\n",
        "\n",
        "exclude=[\"safety_checker/model.safetensors\",\n",
        "        \"unet/diffusion_pytorch_model.safetensors\",\n",
        "        \"text_encoder/model.safetensors\",\n",
        "        \"safety_checker/model.ckpt\",\n",
        "        \"unet/diffusion_pytorch_model.ckpt\",\n",
        "        \"text_encoder/model.ckpt\",\n",
        "                 ]\n",
        "\n",
        "\n",
        "class vae_set(pipeline_setup):\n",
        "    def __init__(self,vae_change,vae_select,base_pipe,hub_select,auto):\n",
        "        self.use_input_url=False\n",
        "        self.vae_change=vae_change\n",
        "        self.base_pipe=base_pipe\n",
        "        self.vae_model_name=vae_select\n",
        "        self.vae_path=\"\"\n",
        "        super().__init__(model_select,device,hub_select,auto)\n",
        "\n",
        "    def vae_name_set(self,vae_model_name):\n",
        "        model_names=self.model_name_search(vae_model_name,True)\n",
        "        vae_path=self.file_name_set(model_names,True)\n",
        "        return model_names , vae_path\n",
        "\n",
        "    def vae_load(self):\n",
        "        global vae\n",
        "        model_names,vae_path=self.vae_name_set(self.vae_model_name)\n",
        "        if self.diffusers_model_check(model_names):\n",
        "            self.use_input_url=False\n",
        "            vae_path=model_names\n",
        "        else:\n",
        "            self.use_input_url=True\n",
        "            vae_path=f\"https://huggingface.co/{model_names}/blob/main/{vae_path}\"\n",
        "        if self.vae_change:\n",
        "            if self.use_input_url:\n",
        "                vae = AutoencoderKL.from_single_file(vae_path)\n",
        "            else:\n",
        "                try:\n",
        "                    vae = AutoencoderKL.from_pretrained(vae_path)\n",
        "                except:\n",
        "                    vae = AutoencoderKL.from_pretrained(vae_path , subfolder=\"vae\")\n",
        "\n",
        "        else:\n",
        "            vae= self.base_pipe.vae\n",
        "        return vae,vae_path\n",
        "\n",
        "\n",
        "\n",
        "hub_select=\"hugface\"\n",
        "vae_setup=vae_set(vae_change,vae_select,base_pipe,hub_select,auto)\n",
        "\n",
        "if key_check(Scheduler_select) and vae_change:\n",
        "    vae,vae_path=vae_setup.vae_load()\n",
        "\n",
        "else:\n",
        "    vae_path=\"\"\n",
        "    vae=base_pipe.vae\n",
        "\n",
        "\n",
        "\n",
        "def Euler_sh_set():\n",
        "    global CustomEuler,CustomEulerA\n",
        "    if not os.path.exists(\"/content/script/Euler_mod\"):\n",
        "        os.makedirs(\"/content/script/Euler_mod\",exist_ok=True)\n",
        "        !git clone https://github.com/alexblattner/modified-euler-samplers-for-sonar-diffusers.git /content/script/Euler_mod\n",
        "    path_1 = \"/content/script/Euler_mod/EulerANew.py\"\n",
        "    path_2 = \"/content/script/Euler_mod/EulerNew.py\"\n",
        "    if path_1 not in sys.path:\n",
        "        sys.path.append(path_1)\n",
        "        sys.path.append(path_2)\n",
        "    os.chdir(\"/content/script/Euler_mod\")\n",
        "    try:\n",
        "        import_on_str(\"EulerA\",module_name=\"EulerANew\",as_arg=\"CustomEulerA\")\n",
        "        import_on_str(\"Euler\",module_name=\"EulerNew\",as_arg=\"CustomEuler\")\n",
        "        #import EulerANew as CustomEulerA\n",
        "        #import EulerNew as CustomEuler\n",
        "        #from EulerANew import EulerA\n",
        "        #from EulerNew import Euler\n",
        "    except:\n",
        "        !python EulerANew.py\n",
        "        !python EulerNew.py\n",
        "        import_on_str(\"EulerA\",module_name=\"EulerANew\",as_arg=\"CustomEulerA\")\n",
        "        import_on_str(\"Euler\",module_name=\"EulerNew\",as_arg=\"CustomEuler\")\n",
        "        #from EulerANew import EulerA\n",
        "        #from EulerNew import Euler\n",
        "    os.chdir(\"/content\")\n",
        "    return CustomEulerA,CustomEuler\n",
        "\n",
        "\n",
        "def Scheduler_set(Scheduler_select):\n",
        "    Scheduler_dict={\n",
        "            \"DDPM\": \"DDPMScheduler\",\n",
        "            \"DDIM\": \"DDIMScheduler\",\n",
        "            \"PNDM\": \"PNDMScheduler\",\n",
        "            \"LMSD\" : \"LMSDiscreteScheduler\",\n",
        "            \"DPM\":  \"DPMSolverMultistepScheduler\",\n",
        "            \"EulerA\": \"EulerAncestralDiscreteScheduler\",\n",
        "            \"Euler\": \"EulerDiscreteScheduler\",\n",
        "            \"DEISM\":\"DEISMultistepScheduler\",\n",
        "            \"UniPCM\":\"UniPCMultistepScheduler\",\n",
        "            \"K_DPM2D\":\"KDPM2DiscreteScheduler\",\n",
        "            \"DPM_S\":\"DPMSolverSinglestepScheduler\",\n",
        "            \"K_DPM2AD\":\"KDPM2AncestralDiscreteScheduler\",\n",
        "            \"HeunD\":\"HeunDiscreteScheduler\",\n",
        "            }\n",
        "    diffusers_module=dir(diffusers)\n",
        "    if Scheduler_select in Scheduler_dict:\n",
        "        Scheduler_select=Scheduler_dict[Scheduler_select]\n",
        "\n",
        "    if Scheduler_select==\"CustomEulerA\" or Scheduler_select==\"CustomEuler\":\n",
        "        CustomEulerA,CustomEuler=Euler_sh_set()\n",
        "    else:\n",
        "        CustomEulerA,CustomEuler=None,None\n",
        "        if Scheduler_select not in diffusers_module:\n",
        "            #Error_txt=max_temper(Scheduler_select, diffusers_module)\n",
        "            _error=make_Error_message(Scheduler_select,dir(diffusers),need_txt=\"Scheduler\")\n",
        "            raise AttributeError(f'\"{Scheduler_select}\" not found. Maybe \"{_error}\" ?')\n",
        "\n",
        "    return Scheduler_select,CustomEulerA,CustomEuler\n",
        "\n",
        "Scheduler_select,CustomEulerA,CustomEuler=Scheduler_set(Scheduler_select)\n",
        "\n",
        "\n",
        "EN='''埋め込み: 有効\\n有効化の鍵: <EasyNegative>,<bad-hands>\\n\n",
        "※埋め込みを使用する場合、「有効化の鍵」を＜＞ごとコピー＆ペーストしてください'''\n",
        "\n",
        "class make_main_pipe:\n",
        "    def __init__(self,device,vae,pipe_set,base_pipe,Scheduler_select):\n",
        "        self.base_pipe=base_pipe\n",
        "        deivce=device\n",
        "        self.Scheduler_select=Scheduler_select\n",
        "        self.pipe_args_setup = {}\n",
        "        self.stetas={}\n",
        "        self.main_name=\"\"\n",
        "        self.vae=vae\n",
        "\n",
        "    def main_pipe_set(self):\n",
        "        txt2img_pipe,img2img_pipe,txt2video_pipe,Inpaint_pipe,safe_pipe=None,None,None,None,None\n",
        "        textual_inversion_dict ={\n",
        "            \"EasyNegativeV2.safetensors\": \"EasyNegative\",\n",
        "            \"bad-hands-5.pt\": \"bad-hands\",\n",
        "            }\n",
        "\n",
        "        #self.Scheduler_class = Scheduler_dict[self.Scheduler_select]\n",
        "        if self.Scheduler_select not in [\"CustomEuler\",\"CustomEulerA\"]:\n",
        "            import_on_str(self.Scheduler_select,\"diffusers\")\n",
        "            self.Scheduler_class = getattr(diffusers, self.Scheduler_select)\n",
        "        else:\n",
        "            self.Scheduler_class = globals().get(self.Scheduler_select)\n",
        "\n",
        "        scheduler = self.Scheduler_class.from_config(self.base_pipe.scheduler.config)\n",
        "        self.stetas[\"vae\"]=self.vae\n",
        "        self.stetas[\"text_encoder\"]=self.base_pipe.text_encoder\n",
        "        self.stetas[\"tokenizer\"]=self.base_pipe.tokenizer\n",
        "        self.stetas[\"unet\"]=self.base_pipe.unet\n",
        "        self.stetas[\"scheduler\"]=scheduler\n",
        "        self.stetas[\"feature_extractor\"]=self.base_pipe.feature_extractor if not Filter_off else None\n",
        "        stetas_check=get_call_method(class_name=pipe_type,method_name='__init__')\n",
        "        if \"safety_checker\" in stetas_check:\n",
        "            self.stetas[\"safety_checker\"]=self.base_pipe.safety_checker if not Filter_off else None\n",
        "        elif Filter_off:\n",
        "            print(f'\"{pipe_type}\"ではsatety_checkerの指定が使用できません')\n",
        "\n",
        "\n",
        "        self.pipe_class=getattr(diffusers, pipe_type)\n",
        "\n",
        "        main_pipe =self.pipe_class(**self.stetas).to(device, torch.float16)\n",
        "        if (not pipe_type==\"StableDiffusionPipelineSafe\") and (token_list != \"EasyNegative\") and (token_list != \"bad-hands\"):\n",
        "            textual_error=False\n",
        "            try:\n",
        "                main_pipe.load_textual_inversion(\"embed/negative\", weight_name=\"EasyNegativeV2.safetensors\", token=\"EasyNegative\")\n",
        "                main_pipe.load_textual_inversion(\"embed/negative\", weight_name=\"bad-hands-5.pt\", token=\"bad-hands\")\n",
        "            except ValueError:\n",
        "                textual_error=True\n",
        "                pass\n",
        "            if not textual_error:\n",
        "                token_list.append(\"EasyNegative\")\n",
        "                token_list.append(\"bad-hands\")\n",
        "            else:\n",
        "                if \"EasyNegative\" or \"bad-hands\" in token_list:\n",
        "                    print(\"既に埋め込みは適用されています。\")\n",
        "                    EN=\"\\033[33m埋め込み: 適用済み\\033[34m\"\n",
        "                else:\n",
        "                    EN=\"\\033[33m埋め込み: このモデルではsd1.5系列の埋め込みは使用不可です\\033[34m\"\n",
        "        else:\n",
        "            EN=\"\\033[33m埋め込み: このモデルでは使用不可です\\033[0m\"\n",
        "        try:\n",
        "            main_pipe.enable_vae_tiling()\n",
        "        except:\n",
        "            pass\n",
        "        return main_pipe\n",
        "\n",
        "MMP=make_main_pipe(device,vae,pipe_set,base_pipe,Scheduler_select)\n",
        "main_pipe=MMP.main_pipe_set()\n",
        "\n",
        "\n",
        "filter_level=result()\n",
        "\n",
        "\n",
        "model_print=(f\"model_path: {model_path}\")\n",
        "\n",
        "def k():\n",
        "    global fin\n",
        "    fin=True\n",
        "    return fin\n",
        "fin=k()\n",
        "#\\033[31mが赤、\\033[33mが黄色、\\033[34mが青、\\033[32mが緑、\\033[0mが白\n",
        "# \"\\033[32m\" は緑色に変更するための\"ANSI Escape Code\"であり、\"\\033[0m\"はデフォルトの文字色に戻すためのコードです。\n",
        "# \\033[38;2;0;255;255m　水色\n",
        "#\\033[38;2;74;229;110m　黄緑\n",
        "if Scheduler_select!=\"Euler+OP\"and Scheduler_select!=\"EulerA+OP\":\n",
        "    print('\\n\\nmomentは、\"Scheduler_selectが \"Euler+OP\"、\"EulerA+OP\"のいずれかでのみ有効です。\\n')\n",
        "\n",
        "print(\"\\033[34m____________________________________________________________________________\\n\\n\")\n",
        "\n",
        "print(f\"model_path: {model_path}\"+\"\\n\\n\")\n",
        "\n",
        "print(\"scheduler:  \"+Scheduler_select+\"\\n\\n\")\n",
        "\n",
        "if vae_change:\n",
        "    print(f\"vae_path: {vae_path}\\n\\n\")\n",
        "\n",
        "print(word_format+\"\\n\")\n",
        "\n",
        "print(filter_level+\"\\n\\n\")\n",
        "\n",
        "\n",
        "print(\"\\033[32m画像生成の準備が出来ました。次の手順に移ってください。\\n\")\n",
        "\n",
        "print(\"(Now that you are ready to generate images, proceed to the next step.)\\033[0m\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Option"
      ],
      "metadata": {
        "id": "ONEtEJyRGYSL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title  (option) Load Lora{display-mode: \"form\"}\n",
        "\n",
        "model_name = \"gura\" # @param [\"stable diffusion-v2.1(basic)\", \"Counterfeit-V2.5(Anime)(better)\", \"loliDiffusion(Anime)\", \"waifu diffusion-v1.4(Anime)\", \"Anything-v3.0(Anime)\", \"Anything-v4.5(Anime)\", \"anything-midjourney-v-4-1(Anime)\", \"ACertainThing(Anime)\", \"anime-kawai-diffusion(Anime)\", \"AB4.5_AC0.2(Anime)\", \"basil_mix(Anime)\", \"Counterfeit(Anime)\", \"Counterfeit-V2.0(Anime)\", \"chilled_remix(Anime)\", \"Double-Exposure-Diffusion(Anime)\", \"EimisAnimeDiffusion_1.0v(Anime)\", \"7th_Layer(Anime)\", \"Riga_Collection(Anime)\", \"Waifu-Diffusers(Anime)\", \"JWST-Deep-Space-diffusion(space)\", \"sd-db-epic-space-machine(space_ship)\", \"spacemidj(space)\", \"nasa-space-v2(space)\", \"openjourney-v4(Reality)\", \"Realistic_Vision_V2.0(Reality)\", \"meinamix_meinaV10(Reality)\", \"search\"] {allow-input: true}\n",
        "del_word_list=[\"(basic)\",\"(Anime)\",\"(Reality)\",\"(space_ship)\",\"(space)\",\"(better)\"]\n",
        "for del_word in del_word_list:\n",
        "    model_name=model_name.replace(del_word, \"\" )\n",
        "hub_select = \"civitai\" #@param [\"hugface\",\"civitai\"]\n",
        "\n",
        "LORA_path=pipe_set.model_set(model_name,\"LORA\",hub_select=hub_select,auto=False)\n",
        "\n",
        "try:\n",
        "    main_pipe.load_lora_weights(LORA_path)\n",
        "except:\n",
        "    ERROR_M=(\"\"\"Loraのロードに失敗しました\n",
        "            原因として、SDXL用のLoraをロードしようとしている可能性もあります\"\"\")\n",
        "    raise ValueError(ERROR_M)\n",
        "\n",
        "print(\"\\033[32mLoraのロードに成功しました\\033[0m\")\n",
        "\n"
      ],
      "metadata": {
        "id": "VmpMRkz2QOyP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title   (option)埋め込みを適用 {display-mode: \"form\"}\n",
        "#@markdown >埋め込みを適用\n",
        "Repo_id_or_path = \"hololive\" # @param {type:\"string\"}\n",
        "weight_name= \"\" # @param {type:\"string\"}\n",
        "token = \"<gura>\" # @param {type:\"string\"}\n",
        "\n",
        "#NEW\n",
        "\n",
        "Loaded_from = \"hugface\" # @param [\"hugface\", \"civitai\", \"local\"]\n",
        "\n",
        "#LORA_path=pipe_set.model_set(model_name,\"TextualInversion\",hub_select=hub_select,auto=False)\n",
        "loaded_textual_list=[]\n",
        "class textual(pipeline_setup):\n",
        "    def __init__(self,pretrained_path,weight,token,main_pipe,loaded_textual_list,hub_select=\"hugface\"):\n",
        "        pretrained_path=pretrained_path\n",
        "        weight=weight\n",
        "        token=token\n",
        "        main_pipe=main_pipe\n",
        "        loaded_textual_list=loaded_textual_list\n",
        "        hub_select=hub_select\n",
        "        #self.special_file=\"\"\n",
        "\n",
        "    def load_textual(self,pretrained_path,weight,token,main_pipe,loaded_textual_list,hub_select=\"hugface\"):\n",
        "        global token_list\n",
        "        if \"token_list\" not in globals():\n",
        "            token_list=[]\n",
        "        state={\"pretrained_model_name_or_path\":pretrained_path,\n",
        "               \"token\":token}\n",
        "        if token in token_list:\n",
        "            raise ValueError(\"適用済みのトークンの為、別のトークンの入力をお願いします\")\n",
        "        elif os.path.isfile(pretrained_path):\n",
        "            pass\n",
        "            #state[\"weight_name\"]=weight_name\n",
        "        else:\n",
        "            embed_path=pipe_set.model_set(pretrained_path,\"TextualInversion\",hub_select=hub_select,auto=False)\n",
        "            if hub_select==\"hugface\":\n",
        "                state[\"weight_name\"]=self.special_file\n",
        "        try:\n",
        "            main_pipe.load_textual_inversion(**state)\n",
        "            token_list.append(token)\n",
        "        except ValueError:\n",
        "            if token in token_list:\n",
        "                print(\"すでに指定されたトークンは適用済みです\")\n",
        "            else:\n",
        "                print(\"カスタム埋め込みの適用に失敗しました\")\n",
        "        print(f\"カスタムトークン: {','.join(token_list)}\\n\")\n",
        "\n",
        "text_c=textual(Repo_id_or_path,weight_name,token,main_pipe,loaded_textual_list,hub_select=Loaded_from)\n",
        "#ct=text_cadd_textual(Repo_id_or_path,weight_name,token,main_pipe)\n",
        "text_c.load_textual(Repo_id_or_path,weight_name,token,main_pipe,loaded_textual_list,hub_select=Loaded_from)\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "_KBSJk1kqN3Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#main step"
      ],
      "metadata": {
        "id": "5qIH-0uIGiw7"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xXSW3nl8UT7x"
      },
      "outputs": [],
      "source": [
        "#@title  #Step.4 画像生成  (iamge generation){display-mode: \"form\"}\n",
        "\n",
        "#@markdown ># 基本設定\n",
        "\n",
        "# @markdown >基本ステータス\n",
        "\n",
        "\n",
        "Prompt = \"Please draw a beautiful Mount Fuji with the sun rising from the summit\" # @param [\"smail,girl, white hair, medium hair, cat ears, looking at viewer, :3, cute,white_dress\", \"smail,1girl, {white,blue,red,purple} hair, medium hair, cat ears, looking at viewer, :3, cute,white_dress,loli\", \"Please draw a beautiful Mount Fuji with the sun rising from the summit\", \"Earth, space, high resolution\"] {allow-input: true}\n",
        "\n",
        "N_prompt = \"logo,text\"  # @param {type:\"string\"}\n",
        "\n",
        "seed = -1 # @param {type:\"number\"}\n",
        "\n",
        "\n",
        "num_imgs = 3 # @param {type:\"number\"}\n",
        "\n",
        "guidance_scale = 7.5 # @param {type:\"slider\", min:5, max:15, step:0.5}\n",
        "\n",
        "\n",
        "# @markdown >追加ステータス\n",
        "\n",
        "\n",
        "拡散ステップ = 30  # @param {type:\"number\"}\n",
        "縦の大きさ = \"512\" #@param [\"480\",\"512\",\"600\", \"768\",\"800\", \"1080\",\"1152\", \"1440\",\"1920\", \"3840\",\"4000\",\"7680\"] {allow-input: true}\n",
        "横の大きさ = \"512\" #@param [\"480\",\"512\",\"600\", \"768\",\"800\", \"1080\",\"1152\", \"1440\", \"1920\", \"3840\",\"4000\",\"7680\"] {allow-input: true}\n",
        "\n",
        "text_generate_model = \"gpt2-prompt-generator\" #@param [\"None\",\"MagicPrompt-Stable-Diffusion\",\"anime-anything-promptgen-v2\",\"gpt2-prompt-generator\"]\n",
        "\n",
        "#@markdown ># 詳細設定\n",
        "\n",
        "# @markdown >出力設定\n",
        "\n",
        "グリッドの横の画像数 = 6 # @param {type:\"slider\", min:1, max:10, step:1}\n",
        "num_grid=int(グリッドの横の画像数)\n",
        "\n",
        "保存する先のパス = \"/content/drive/MyDrive/生成した画像/No.1\" # @param [\"/content/drive/MyDrive/\"] {allow-input: true}\n",
        "保存する先のパス = 保存する先のパス.strip()\n",
        "\n",
        "file_name = \"\" #@param {type:\"string\"}\n",
        "\n",
        "\n",
        "# @markdown >追加機能\n",
        "推奨設定 = True #@param {type:\"boolean\"}\n",
        "Recommend=推奨設定\n",
        "画像を表示 = True  # @param {type:\"boolean\"}\n",
        "グリッド画像を表示 = True  # @param {type:\"boolean\"}\n",
        "画像情報を表示 = True  # @param {type:\"boolean\"}\n",
        "条件をメタデーターとして追加する = True  # @param {type:\"boolean\"}\n",
        "\n",
        "\n",
        "#NP制限トークン数拡大 = True #@param {type:\"boolean\"}\n",
        "\n",
        "\n",
        "日本語入力 = False #@param {type:\"boolean\"}\n",
        "JP_INPUT=日本語入力\n",
        "\n",
        "\n",
        "警告メッセージを非表示 = False #@param {type:\"boolean\"}\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "if (推奨設定 and\n",
        " (momentum-momentum_hist)<=0.3 and\n",
        " (Scheduler_select==\"EulerA+OP\" or Scheduler_select==\"Euler+OP\")):\n",
        "        momentum_hist=momentum-0.3\n",
        "        print(f\"momentum_histとmomentumの差が3以下のためmomentum_histを'{momentum_hist}' に設定しました。\\nこの処理は、'推奨設定'に付属している機能です\")\n",
        "\n",
        "\n",
        "sd_step=拡散ステップ\n",
        "\n",
        "height = int(縦の大きさ)\n",
        "width = int(横の大きさ)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "a1=time.time()\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "if \"can_EN\" not in locals():\n",
        "    can_EN = False\n",
        "\n",
        "if \"text_generate_model\" not in locals():\n",
        "    text_generate_model=\"None\"\n",
        "\n",
        "\n",
        "if (not 推奨設定) and (not 警告メッセージを非表示):\n",
        "    print('\\033[33m\"推奨設定\"がオフになっています。')\n",
        "\n",
        "\n",
        "\n",
        "def run_html_js(path,moji):\n",
        "    import datetime\n",
        "    num=len(path)\n",
        "    now_a = datetime.datetime.now()\n",
        "    datetimes = now_a.strftime(\"%Y%m%d%H%M%S\")\n",
        "    html_dis = f'''\n",
        "    <style>\n",
        "      #clipborad-text-{datetimes} {{\n",
        "        border: none;\n",
        "        color: #0ff;\n",
        "        font-size: 15px;\n",
        "      }}\n",
        "    </style>\n",
        "    <span style=\"color: #4ae56e\" font-size:16px>{moji}</span> <!-- <p>タグを<span>タグに変更 -->\n",
        "    <input type=\"text\" value=\"{path}\" id=\"clipborad-text-{datetimes}\" size=\"{num}\" readonly> <!-- size属性とid属性を追加 -->\n",
        "    <button id=\"copy-button-{datetimes}\" onclick=\"copyToClipboard('{datetimes}')\">Copy</button> <!-- id属性とonclick属性を追加 -->\n",
        "    '''\n",
        "    js_code = '''\n",
        "    function copyToClipboard(datetimes) {\n",
        "      var copyText = document.getElementById(\"clipborad-text-\" + datetimes); // datetimeを結合\n",
        "      var copyButton = document.getElementById(\"copy-button-\" + datetimes); // datetimeを結合\n",
        "      copyText.select();\n",
        "      navigator.clipboard.writeText(copyText.value);\n",
        "      document.execCommand(\"copy\");\n",
        "      copyButton.textContent = \"Copied!\"; // ボタンの文字を変更\n",
        "      setTimeout(function() {\n",
        "        copyButton.textContent = \"Copy\"; // 1秒後に元に戻す\n",
        "        }, 1000);\n",
        "    }\n",
        "    '''\n",
        "    display(HTML(html_dis))\n",
        "    display(HTML('<script>{}</script>'.format(js_code)))\n",
        "\n",
        "\n",
        "def img_set(path):\n",
        "    #対策↓:AttributeError: 'str' object has no attribute 'seek'\n",
        "    if os.path.isfile(path):\n",
        "        init_image = Image.open(path)\n",
        "        init_image = init_image.resize((height, width))\n",
        "    else:\n",
        "        init_image = None\n",
        "        if not pipe_type==\"txt2img\"or\"safe\":\n",
        "            raise FileNotFoundError('\\033[31m\"入力する画像\"のpathが存在しません。使用しない場合は空白にお願いします\\033[0m')\n",
        "    return init_image\n",
        "\n",
        "\n",
        "class Scheduler_setup(make_main_pipe):\n",
        "    def __init__(self,main_pipe):\n",
        "        self.main_pipe=main_pipe\n",
        "\n",
        "    def moment_set(self,Scheduler_select,vae):\n",
        "        if Scheduler_select==\"Euler+OP\" or Scheduler_select==\"EulerA+OP\":\n",
        "            self.main_pipe.scheduler.history_d =history_d\n",
        "            self.main_pipe.scheduler.momentum = momentum\n",
        "            self.main_pipe.scheduler.momentum_hist = momentum_hist\n",
        "sc_set=Scheduler_setup(main_pipe)\n",
        "sc_set.moment_set(Scheduler_select,vae)\n",
        "if (not Prompt.endswith(\",\")) and (Prompt is not None):\n",
        "    Prompt=Prompt+\",\"\n",
        "\n",
        "img_dir=False\n",
        "img_dir_num=1\n",
        "img_path=\"\"\n",
        "img_dict = []\n",
        "if pipe_type==\"img2img\":\n",
        "    if os.path.isfile(入力する画像) and 入力する画像.endswith((\".png\", \".jpg\", \".jpeg\")):\n",
        "        img_path=入力する画像\n",
        "    elif os.path.isdir(入力する画像):\n",
        "        files = os.listdir(入力する画像)\n",
        "        img_ext = [\".jpg\", \".png\",\".jpeg\"]\n",
        "        for file in files:\n",
        "            ext = os.path.splitext(file)[1]\n",
        "            if ext in img_ext:\n",
        "                img_path = os.path.join(入力する画像, file)\n",
        "                img_dict.append(img_path)\n",
        "        if img_dict:\n",
        "            img_dir=True\n",
        "            img_dir_num=len(img_dict)\n",
        "        else:\n",
        "            raise FileNotFoundError(\"指定されたフォルダ内に画像が見つかりませんでした\")\n",
        "    else:\n",
        "        raise FileNotFoundError('入力する画像が見つかりませんでした。画像を入力しない場合は、Step.1のパイプラインの種類を\"txt2img\"に変更お願いします')\n",
        "\n",
        "if not img_dir:\n",
        "    num=num_imgs\n",
        "else:\n",
        "    num=num_imgs*(len(img_dict))\n",
        "\n",
        "if JP_INPUT:\n",
        "    tr = Translator()\n",
        "    Prompt_2_a = tr.translate(Prompt, src=\"ja\", dest=\"en\").text\n",
        "    Prompt_2=Prompt_2_a\n",
        "else:\n",
        "    Prompt_2=Prompt\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def txt_pipe(text_generate_model):\n",
        "    global nlp\n",
        "    txt_pipe_dict={\"MagicPrompt-Stable-Diffusion\":\"Gustavosta/MagicPrompt-Stable-Diffusion\",\n",
        "                   \"anime-anything-promptgen-v2\":\"FredZhang7/anime-anything-promptgen-v2\",\n",
        "                   \"gpt2-prompt-generator\":\"Ar4ikov/gpt2-medium-650k-stable-diffusion-prompt-generator\",\n",
        "                   \"None\":\"None\"}\n",
        "\n",
        "\n",
        "    txt_model_name = txt_pipe_dict[text_generate_model]\n",
        "    if \"nlp\" not in globals():\n",
        "        nlp=None\n",
        "\n",
        "    if txt_model_name!=\"None\" and (not key_check(txt_model_name)):\n",
        "        model = AutoModelForCausalLM.from_pretrained(txt_model_name)\n",
        "        tokenizer = AutoTokenizer.from_pretrained(txt_model_name)\n",
        "        nlp = pipeline('text-generation', model=model, tokenizer=tokenizer, device=device, pad_token_id=50256)\n",
        "    return nlp\n",
        "nlp=txt_pipe(text_generate_model)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "txt_generator = torch.Generator(device)\n",
        "\n",
        "\n",
        "\n",
        "class txt_model_setup:\n",
        "    def __init__(self,nlp,Prompt,Prompt_2,add_good_prompt,JP_INPUT,text_generate_model,N_prompt):\n",
        "        global Prompt_list , make_images_list\n",
        "        Prompt_list=[]\n",
        "        make_images_list=[]\n",
        "        self.text_model_name=None\n",
        "        self.pipe_name=None\n",
        "        Prompt=Prompt\n",
        "        self.N_prompt=N_prompt\n",
        "        Prompt_2=Prompt_2\n",
        "        self.add_good_prompt=推奨設定\n",
        "        self.tokens=None\n",
        "        JP_INPUT=JP_INPUT\n",
        "        self.txt_model_name=\"\"\n",
        "        self.txt_pipe_name=\"\"\n",
        "        nlp=nlp\n",
        "        self.good_word = \"masterpiece:2.0,best quality,high quality,\"\n",
        "        text_generate_model=text_generate_model\n",
        "        if self.add_good_prompt:\n",
        "            self.mini_prompt=True\n",
        "        else:\n",
        "            self.mini_prompt=False\n",
        "\n",
        "\n",
        "\n",
        "    def get_tokens_as_list(self,word_list):\n",
        "        tokens_list = []\n",
        "        for word in word_list:\n",
        "            tokenized_word = self.tokenizer_with_prefix_space([word], add_special_tokens=False).input_ids[0]\n",
        "            tokens_list.append(tokenized_word)\n",
        "        return tokens_list\n",
        "\n",
        "    def p_token(self,Prompt):\n",
        "        self.tokens = int(len(self.tokenizer.tokenize(Prompt)))\n",
        "        return self.tokens\n",
        "    def txt_G(self,nlp,Prompt_2):\n",
        "        n_word=[\"EasyNegative\",\"bat_hands\",\"white background\",\" simple background\",\"  simple background\",\"Loss of eye highlights\",\"simple background\",\"Fingers fused together\",\"Writing Sweet Fingers\",\"bad hands\",\"bad legs\",\"worst quality\",\"low quality\",\"Not five fingers\",\"blurred\",\"Missing finger\",\"Simple background\",\"Cat with deformed face\",\"medium quality\",\"purple hair\",\"Loss of eye highlights\",\"Fingers fused together\",\"Writing Sweet Fingers\",\"deleted\",\"lowres\",\"Low quality animals\",\"deformed animals\",\"hands emerging from impossible places\",\"bad anatomy\",\"more than three limbs hands/legs\",\"low resolution\",\"blurry\",\"absurdres\",\"pixelated\",\"sketchy\",\"nonsensical anatomy\",\"unrealistic pose\",\"mosaic\",\"unclear details\",\"distorted colors\",\"unrealistic proportions\",\"poor quality\",\"fuzzy\",\"missing head:1.6\",\"out of focus\",\"hazy\",\"grainy\",\"text\",\"error\",\"missing fingers:0.9\",\"extra digit\",\"fewer digits\",\"cropped\",\"jpeg artifacts\",\"signature\",\"watermark\",\"username\",\"standard quality\",\"bad feet_hand_finger_leg_eye\",\"bad\",\"text font ui\",\"bad shadow\",\"poorly drawn\",\"black-white\",\"ugly\",\"duplicate\",\"mutation\",\"mutilated\",\"malformed mutated:1.1\",\"malformed:1.1\",\"The background is incoherent\",\"simple background\",\"low-quality background\",\"low background\",\"bad body\",\"long body\",\"broken limb\",\"anatomical nonsense\",\"extra limbs\",\"missing limb\",\"incorrect limb\",\"multiple heads\",\"twisted head\",\"poorly drawn face\",\"1 unit with multiple heads:1.3\",\"heads together:1.0\",\"abnormal eye:1.2 proportion\",\"cropped:1.0\",\"bad eyes\",\"fused eyes\",\"poorly drawn eyes\",\"bad mouth\",\"poorly drawn mouth\",\"bad tongue\",\"too long tongue\",\"bad ears\",\"poorly drawn ears\",\"extra ears\",\"heavy ears\",\"long neck\",\"too thick neck\",\"bad neck\",\"bad breasts\",\"missing arms\",\"disappearing arms\",\"extra arms\",\"three arms:2.0\",\"mutated hands and fingers\",\"fused hand\",\"missing fingers\",\"extra digits\",\"huge thighs\",\"disappearing thigh\",\"missing thighs\",\"extra thighs\",\"bad feet\",\"huge calf\",\"disappearing legs\",\"bad gloves\",\"fused gloves\",\"beard\",\"artist name\",\"text watermark\",\"unnatural\",\"obviously wrong\",\"distorted face\",\"floating hair\",\"floating body parts\",\"severed body parts\",\"incorrect leg position\",\"deformed\",\"fused body and hands\",\"disregard of physics\",\"distorted shape\",\"doll-like object not present in the image\",\"body fusion\",\"abnormal fingers\",\"fingers resembling fish fins\",\"dot eyes\",\"unclear background\",\"mosaic\",\"body bending\",\"incorrect leg-to-torso ratio\",\"excessively large breasts\",\"unsettling appearance\",\"eyes filled with solid color\",\"lack of lower body\",\"splitting\",\"creepy doll-like appearance\",\"distorted eyes\",\"lines on the skin\",\"legs bending in unnatural directions\",\"abnormal finger count\",\"missing arms\",\"floating hands\",\"lack of nose or mouth\",\"incorrect body part ratios\",\"bad\",\"longbody\",\"lowres\",\"bad anatomy\",\"bad hands\",\"missing fingers\",\"Distorted eye contour\",\"Missing part from the ankles onward\",\"extra digit\",\"fewer digits\",\"split wings\",\"Vampire wings floating in the air\",\"bad wing\",\"wonder egg priority\",\"egg priority\",\"demon\"]\n",
        "        if not N_prompt==\"\":\n",
        "            words = N_prompt.split(\",\")\n",
        "            n_word.extend(words)\n",
        "        if self.mini_prompt==True:\n",
        "            self.max_length=64\n",
        "        else:\n",
        "            self.max_length=74\n",
        "\n",
        "        Prompt_4_M = nlp(Prompt_2, max_length=self.max_length, num_return_sequences=10 ,repetition_penalty=1.2, early_stopping=False ,do_sample=True, temperature=0.4, top_k=10)\n",
        "        for V in range(len(Prompt_4_M)):\n",
        "            Prompt_4_M[V] = str(Prompt_4_M[V]['generated_text']).replace('  ', '').lstrip(',')\n",
        "        Prompt_4_I=''.join(Prompt_4_M)\n",
        "        Prompt_4_I=Prompt_4_M[0]\n",
        "        af_text = Prompt_4_I.replace(\",,\", \",\").replace(\",,,\", \",\").replace(\", ,\",\",\")\n",
        "        if self.mini_prompt==True:\n",
        "            Prompt_4=self.good_word+af_text\n",
        "        else:\n",
        "            Prompt_4=af_text\n",
        "        return Prompt_4\n",
        "\n",
        "    def easy_prompt(self,nlp,Prompt_2,text_generate_model):\n",
        "        txt_pipe_dict={\"MagicPrompt-Stable-Diffusion\":(\"Gustavosta/MagicPrompt-Stable-Diffusion\",\"MagicPrompt_base\",\"MagicPrompt_tokenizer\",\"MagicPrompt_with\"),\n",
        "                       \"anime-anything-promptgen-v2\":(\"FredZhang7/anime-anything-promptgen-v2\",\"AnythingPrompt_base\",\"AnythingPrompt_tokenizer\",\"AnythingPrompt_with\"),\n",
        "                       \"gpt2-prompt-generator\":(\"Ar4ikov/gpt2-medium-650k-stable-diffusion-prompt-generator\",\"gpt2-prompt-generator_base\",\"gpt2-prompt-generator_tokenizer\",\"gpt2-prompt-generator_with\"),\n",
        "                       \"None\":(\"\",\"\",\"\",\"\")\n",
        "                       }\n",
        "        self.txt_model_name,self.model,self.tokenizer,self.tokenizer_with_prefix_space = txt_pipe_dict[text_generate_model]\n",
        "        Prompt_2_G=str(Prompt_2)\n",
        "        contents = re.findall(\"\\{(.*?)\\}\", Prompt_2_G)\n",
        "        count = 0\n",
        "        for content in contents:\n",
        "            if not content or len(content) == 0:\n",
        "                continue\n",
        "            lst = content.split(\",\")\n",
        "            choice = random.choice(lst)\n",
        "            Prompt_2_G = re.sub(\"\\{.*?\\}\", choice, Prompt_2_G, count=1)\n",
        "            count += 1\n",
        "        if text_generate_model == \"None\":\n",
        "            self.good_word = \"masterpiece:2.0,best quality,high quality,\"\n",
        "            text_model_name =\"\"\n",
        "            if self.mini_prompt:\n",
        "                Prompt_4=self.good_word+Prompt_2_G\n",
        "            else:\n",
        "                Prompt_4 = Prompt_2_G\n",
        "        else:\n",
        "            Prompt_4 = self.txt_G(nlp,Prompt_2_G)\n",
        "        return Prompt_4\n",
        "\n",
        "\n",
        "    def main_task(self,nlp,Prompt_2):\n",
        "        Prompt_4=self.easy_prompt(nlp,Prompt_2,text_generate_model)\n",
        "        return Prompt_4\n",
        "    def sub_task(self,nlp,Prompt_2,Prompt_list,num):\n",
        "        for U in range(num):\n",
        "            Prompt_4=self.main_task(nlp,Prompt_2)\n",
        "            Prompt_list.append(Prompt_4)\n",
        "\n",
        "\n",
        "    def use_over_token(self,PROMPT,pipe):\n",
        "        max_length = pipe.tokenizer.model_max_length\n",
        "        input_ids = pipe.tokenizer(PROMPT, return_tensors=\"pt\").input_ids\n",
        "        input_ids = input_ids.to(\"cuda\")\n",
        "\n",
        "        negative_ids = pipe.tokenizer(\"\", truncation=False, padding=\"max_length\", max_length=input_ids.shape[-1], return_tensors=\"pt\").input_ids\n",
        "        negative_ids = negative_ids.to(\"cuda\")\n",
        "\n",
        "        concat_embeds = []\n",
        "        neg_embeds = []\n",
        "        for i in range(0, input_ids.shape[-1], max_length):\n",
        "            concat_embeds.append(pipe.text_encoder(input_ids[:, i: i + max_length])[0])\n",
        "            neg_embeds.append(pipe.text_encoder(negative_ids[:, i: i + max_length])[0])\n",
        "\n",
        "        #prompt_embeds = torch.cat(concat_embeds, dim=1)\n",
        "        negative_prompt_embeds = torch.cat(neg_embeds, dim=1)\n",
        "        return negative_prompt_embeds\n",
        "\n",
        "\n",
        "tms=txt_model_setup(nlp,Prompt,Prompt_2,Recommend,JP_INPUT,text_generate_model,N_prompt)\n",
        "CPU_P_2 = threading.Thread(target=tms.sub_task,args=(nlp,Prompt_2,Prompt_list,num))\n",
        "if pipe_type==\"txt2video\":\n",
        "    Prompt_4= tms.main_task(nlp,Prompt_2)\n",
        "else:\n",
        "    CPU_P_2.start()\n",
        "    Prompt_4=\"\"\n",
        "\n",
        "\n",
        "\n",
        "if 推奨設定:\n",
        "    おすすめのネガティブプロント = \"Loss of eye :1.5,Fingers fused together:1.3,Writing Sweet Fingers:2.0,bad hands:2.0,bad legs:2.0,EasyNegative,bat_hands:1.3,worst quality:2.0, low quality:2.0,Fused fingers,7 fingers, 6 fingers, 4 fingers, 3 fingers,Not five fingers:2.0,blurred,Simple_background:2.0,Missing finger:1.7,Cat with deformed face:1.3 ,medium quality, purple hair,Loss of eye highlights:1.5,Fingers fused together:1.3,Writing Sweet Fingers:2.0 ,deleted:0.5, lowres,Low quality animals, deformed animals ,hands emerging from impossible places:1.7, bad anatomy, more than three limbs hands/legs:1.5, low resolution, blurry, absurdres,pixelated, sketchy, nonsensical anatomy, unrealistic pose, mosaic, unclear details, distorted colors, unrealistic proportions, poor quality, fuzzy, missing head:1.6, out of focus, hazy, grainy, text, error, missing fingers:0.9, extra digit, fewer digits, cropped, jpeg artifacts, signature, watermark, username, standard quality, bad feet_hand_finger_leg_eye, bad, text font ui, bad shadow, poorly drawn, black-white, ugly, duplicate, mutation, mutilated, malformed mutated:1.1, malformed:1.1, The background is incoherent, simple background, low-quality background, low background, bad body, long body, broken limb, anatomical nonsense, extra limbs, missing limb, incorrect limb, multiple heads, twisted head, poorly drawn face, 1 unit with multiple heads:1.3, heads together:1.0, abnormal eye:1.2 proportion, cropped:1.0, bad eyes, fused eyes, poorly drawn eyes, bad mouth, poorly drawn mouth, bad tongue, too long tongue, bad ears, poorly drawn ears, extra ears, heavy ears, long neck, too thick neck, bad neck,  bad breasts, missing arms, disappearing arms, extra arms, three arms:2.0, mutated hands and fingers, fused hand, missing fingers, extra digits, huge thighs, disappearing thigh, missing thighs, extra thighs, bad feet, huge calf, disappearing legs, bad gloves, fused gloves, beard, artist name, text watermark, unnatural, obviously wrong, distorted face, floating hair, floating body parts, severed body parts, incorrect leg position, deformed, fused body and hands, disregard of physics, distorted shape, doll-like object not present in the image, body fusion, abnormal fingers, fingers resembling fish fins, dot eyes, unclear background, mosaic, body bending, incorrect leg-to-torso ratio, excessively large breasts, unsettling appearance, eyes filled with solid color, lack of lower body, splitting, creepy doll-like appearance, distorted eyes, lines on the skin, legs bending in unnatural directions, abnormal finger count, missing arms, floating hands, lack of nose or mouth,, incorrect body part ratios, bad, longbody, lowres, bad anatomy, bad hands, missing fingers,  Distorted eye contour, Missing part from the ankles onward, extra digit, fewer digits, split wings, Vampire wings floating in the air, bad wing, comic,chainsaw man,demon, simple background\"\n",
        "    if N_prompt:\n",
        "        おすすめのネガティブプロント=\",\"+おすすめのネガティブプロント\n",
        "\n",
        "else:\n",
        "    おすすめのネガティブプロント =\"\"\n",
        "    if not 警告メッセージを非表示:\n",
        "        print('\\033[33m\"推奨ネガティブプロンプト\"がオフになっています。\\033[0m')\n",
        "\n",
        "\n",
        "negative_prompt2 = N_prompt + おすすめのネガティブプロント\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def image_grid(imgs,cols): #縦よこ\n",
        "    all_num=len(imgs)\n",
        "    if all_num<cols:\n",
        "        cols=all_num\n",
        "    if all_num>1:\n",
        "        am=0\n",
        "        w, h = imgs[0].size\n",
        "        rows,b= divmod(all_num,cols)\n",
        "        if b!=0:\n",
        "            rows+=1\n",
        "            am=cols-b\n",
        "            white_img = Image.new(\"RGB\", (w,h), (255, 255, 255))\n",
        "            for x in range(am):\n",
        "                imgs.append(white_img)\n",
        "        grid = Image.new('RGB', size=(cols*w, rows*h))\n",
        "        grid_w, grid_h = grid.size\n",
        "        for i, img in enumerate(imgs):\n",
        "            grid.paste(img, box=(i%cols*w, i//cols*h))\n",
        "    elif all_num==1:\n",
        "        grid=None\n",
        "        print(\"画像が1枚のためグリッド画像化できません\")\n",
        "    else:\n",
        "        grid=None\n",
        "        print(\"グリッド画像に使用可能な画像がありません。\")\n",
        "    return grid\n",
        "\n",
        "\n",
        "if \"<<n>>\" in 保存する先のパス:\n",
        "    for nu in range(1,100000):\n",
        "        save_dir_path, save_file_path=os.path.split(保存する先のパス)\n",
        "        _test_save_file_path = save_file_path.replace(\"<<n>>\", str(nu))\n",
        "        if not os.path.exists(_test_save_file_path):\n",
        "            保存する先のパス= os.path.join(save_dir_path,_test_save_file_path)\n",
        "            break\n",
        "\n",
        "if \"<<a>>\" in 保存する先のパス:\n",
        "    for nu in range(1,100000):\n",
        "        save_dir_path, save_file_path=os.path.split(保存する先のパス)\n",
        "        _test_save_file_path = save_file_path.replace(\"<<a>>\", str(nu))\n",
        "        if \"alls\" in globals():\n",
        "            save_file_path=save_file_path.replace(\"<<a>>\", str(alls))\n",
        "            保存する先のパス= os.path.join(save_dir_path,save_file_path)\n",
        "            break\n",
        "        elif not os.path.exists(_test_save_file_path):\n",
        "            保存する先のパス= os.path.join(save_dir_path,_test_save_file_path)\n",
        "            alls=nu\n",
        "            break\n",
        "\n",
        "\n",
        "if not 保存する先のパス:\n",
        "    保存する先のパス=\"/content/Generated_images\"\n",
        "    print(\"保存する先のパスが未入力のため、デフォルトのパスに保存しました。\")\n",
        "if not conect_drive:\n",
        "    if \"/content/drive/MyDrive\" in 保存する先のパス:\n",
        "      保存する先のパス=\"/content/Generated_images\"\n",
        "      print(\"\\033[31mGoogleドライブに接続されていないためデフォルトのパスに保存しました\\033[0m\")\n",
        "    else:\n",
        "        if not 保存する先のパス:\n",
        "            保存する先のパス=\"/content/Generated_images\"\n",
        "\n",
        "\n",
        "image_save_path=os.path.join(保存する先のパス,\"Images\")\n",
        "\n",
        "Grid_save_path=os.path.join(保存する先のパス,\"Grid\")\n",
        "os.makedirs(image_save_path, exist_ok=True)\n",
        "os.makedirs(Grid_save_path, exist_ok=True)\n",
        "\n",
        "\n",
        "def seed_set(seed_number):\n",
        "    if seed_number==-1 or seed_number is None:\n",
        "        seed = random.randint(1,1000000)\n",
        "    else:\n",
        "        seed=seed_number\n",
        "    return seed\n",
        "seed=seed_set(seed)\n",
        "\n",
        "\n",
        "def path_token_video(Prompt_4,seed_number,model_name,guidance_scale,file_name,image_save_path):\n",
        "    global L\n",
        "    try:\n",
        "        L+=1\n",
        "    except:\n",
        "        L=1\n",
        "    file_name_old=\"\"\n",
        "    if not file_name:\n",
        "        file_name=(\"Gvideo\")\n",
        "    now = datetime.datetime.now()\n",
        "    file_name_encoded = codecs.encode(file_name, 'utf-8')\n",
        "    file_name_decoded = codecs.decode(file_name_encoded, 'utf-8')\n",
        "    path_d = {\"{prompt}\":Prompt_4,\"{seed}\": seed, \"{model_name}\": model_name, \"{g_scale}\": guidance_scale,\"{time}\":now ,\"{number}\":L}\n",
        "    for key, value in path_d.items():\n",
        "        file_name_old = file_name_decoded.replace(key, str(value))\n",
        "    while True:\n",
        "        file_new=(f\"{file_name_old}-{L}.mp4\")\n",
        "        file_test = os.path.join(image_save_path , file_new)\n",
        "        if os.path.exists(file_test):\n",
        "            L+=1\n",
        "        else:\n",
        "           path_new=file_test\n",
        "           file_new=file_new\n",
        "           break\n",
        "    return path_new,file_new\n",
        "\n",
        "\n",
        "def path_token_img(Prompt_4,seed_number,model_name,guidance_scale,file_name,image_save_path):\n",
        "    global z\n",
        "    try:\n",
        "        z+=1\n",
        "    except:\n",
        "        z=1\n",
        "    file_name_old=\"\"\n",
        "    if not file_name:\n",
        "        file_name=(\"GIMG\")\n",
        "    now = datetime.datetime.now()\n",
        "    file_name_encoded = codecs.encode(file_name, 'utf-8')\n",
        "    file_name_decoded = codecs.decode(file_name_encoded, 'utf-8')\n",
        "    path_d = {\"{prompt}\":Prompt_4,\"{seed}\": seed, \"{model_name}\": model_name, \"{g_scale}\": guidance_scale,\"{time}\":now ,\"{number}\":z}\n",
        "    for key, value in path_d.items():\n",
        "        file_name_old = file_name_decoded.replace(key, str(value))\n",
        "    while True:\n",
        "        file_new=(f\"{file_name_old}-{z}.png\")\n",
        "        file_test = os.path.join(image_save_path , file_new)\n",
        "        if os.path.exists(file_test):\n",
        "            z+=1\n",
        "        else:\n",
        "            path_new=file_test\n",
        "            file_new=file_new\n",
        "            break\n",
        "    return path_new,file_new\n",
        "\n",
        "\n",
        "\n",
        "def play_mp4(path_new):\n",
        "    mp4 = open(path_new, 'rb').read()\n",
        "    data_url = 'data:video/mp4;base64,' + b64encode(mp4).decode()\n",
        "    HTML_code=(f\"\"\"\n",
        "               <video width=\"70%\" height=\"70%\" controls>\n",
        "                     <source src=\"{data_url}\" type=\"video/mp4\">\n",
        "               </video>\"\"\")\n",
        "    try:\n",
        "        display(HTML(HTML_code))\n",
        "    except:\n",
        "        display(HTML('<script>{}</script>'.format(HTML_code)))\n",
        "\n",
        "\n",
        "\n",
        "generator = torch.Generator(device)\n",
        "\n",
        "\n",
        "class generate_class:\n",
        "    def __init__(self,main_pipe,Prompt_list,make_images_list,Grid_save_path,img_dir,img_path,img_dict,seed,generator,num_grid,Prompt_4,image_save_path,sd_step,model_name,file_name):\n",
        "        seed=seed\n",
        "        #main_pipe=main_pipe\n",
        "        generator=generator\n",
        "        self.num_grid=num_grid\n",
        "        self.st=num\n",
        "        self.Prompt_4=Prompt_4\n",
        "        Prompt_4=Prompt_4\n",
        "        image_save_path=image_save_path\n",
        "        self.image_save_path=image_save_path\n",
        "        sd_step=sd_step\n",
        "        model_name=model_name\n",
        "        file_name=file_name\n",
        "        self.num_imgs=num_imgs\n",
        "        grid_img=None\n",
        "        self.img_dir=img_dir\n",
        "        self.img_path=img_path\n",
        "        self.img_dict=img_dict\n",
        "        self.Grid_save_path=Grid_save_path\n",
        "        self.init_image=None\n",
        "        self.grid_imgs=[]\n",
        "        self.generate_time_all_after=0\n",
        "        self.nsfw_content_detected=None\n",
        "        self.generate_time_all=0\n",
        "        self.generate_start_time=0\n",
        "        self.generate_end_time=0\n",
        "        self.i=0\n",
        "        self.CPU_P_do=False\n",
        "        self.CPU_P=None\n",
        "        self.GPU_P=None\n",
        "        Prompt_list,make_images_list=Prompt_list,make_images_list\n",
        "        self.generate_num=1\n",
        "        self.Prompt_use=\"\"\n",
        "        self.call_method=get_call_method(pipe_type,method_name='__call__')\n",
        "\n",
        "    def generate_images(self,Prompt_list,make_images_list,seed,generator,image_save_path,sd_step,model_name,file_name,main_pipe):\n",
        "        for self.j in range(self.st):\n",
        "                #生成されない時ここのインデントを確認する\n",
        "                while True:\n",
        "                    if len(Prompt_list)>0:\n",
        "                        self.Prompt_use=Prompt_list.pop(0)\n",
        "                        break\n",
        "                    else:\n",
        "                        time.sleep(0.5)\n",
        "                        #continueがついていると、生成されない\n",
        "                Prompt_use=self.Prompt_use\n",
        "                path_new,file_new=path_token_img(Prompt_use,seed,model_name,guidance_scale,file_name,image_save_path)\n",
        "                if seed==-1:\n",
        "                    seed=seed_set(seed)\n",
        "                now = datetime.datetime.now()\n",
        "                date_str = now.strftime(\"%Y-%m-%d_UTC-%H:%M:%S\")\n",
        "                path = os.path.join(image_save_path, file_name)\n",
        "                generate_start_time = time.time()\n",
        "                generator.manual_seed(seed)\n",
        "\n",
        "                call_dict={}\n",
        "                if \"image\" in self.call_method:\n",
        "                    call_dict[\"image\"]=self.init_image\n",
        "                    call_dict[\"strength\"]=strength\n",
        "                else:\n",
        "                    call_dict[\"height\"]=height\n",
        "                    call_dict[\"width\"]=width\n",
        "                output = main_pipe(prompt=Prompt_use, negative_prompt=negative_prompt2, guidance_scale=guidance_scale, num_inference_steps=sd_step,generator=generator,**call_dict)\n",
        "\n",
        "                self.nsfw_content_detected=output.nsfw_content_detected\n",
        "                make_image=output.images[0]\n",
        "                if self.nsfw_content_detected is None:\n",
        "                    bloke=False\n",
        "                else:\n",
        "                    if False in self.nsfw_content_detected:\n",
        "                        bloke=False\n",
        "                    else:\n",
        "                        bloke=True\n",
        "                info = make_image.info\n",
        "                metadata = {\n",
        "                    \"Seed\": seed,\n",
        "                    \"G_scale\":guidance_scale,\n",
        "                    \"D_step\":sd_step,\n",
        "                    \"model_path\":model_path,\n",
        "                    \"Prompt\": Prompt_use,\n",
        "                    \"n_prompt\":negative_prompt2,\n",
        "\n",
        "                    }\n",
        "\n",
        "\n",
        "                generate_end_time = time.time()\n",
        "                self.generate_time = generate_end_time - generate_start_time\n",
        "                self.generate_time_all += self.generate_time\n",
        "                self.generate_time_after=(\"{:.2f}s\".format(self.generate_time))\n",
        "                #if 枚数制限なし:\n",
        "                #    text_0=(f\"\\033[34m画像生成が完了しました ({self.generate_num}/∞)  {self.generate_time_after}\")\n",
        "                #else:\n",
        "                text_0=(f\"\\033[34m画像生成が完了しました ({self.generate_num}/{self.st})  {self.generate_time_after}\")\n",
        "                text_1=(f\"\"\"{text_0}\\nseed値:\\033[38;2;0;255;255m {seed}\\n\\033[34mファイルの名前: \\033[32m{file_new}\\n\\033[0m\"\"\")\n",
        "                text_2=(f\"\\033[92mプロンプト:  {Prompt_use}\\033[0m\")\n",
        "                make_images_list.append([make_image,path_new,bloke,metadata,text_1,text_2])\n",
        "                if not bloke:\n",
        "                    self.grid_imgs.append(make_image)\n",
        "                else:\n",
        "                    pass\n",
        "                self.generate_num+=1\n",
        "\n",
        "    def last_task(self,make_images_list):\n",
        "        for T in range(self.st):\n",
        "            while True:\n",
        "                if len(make_images_list)>0:\n",
        "                    make_image,path_new,bloke,metadata,text_1,text_2=make_images_list.pop(0)\n",
        "                    break\n",
        "                else:\n",
        "                    time.sleep(0.5)\n",
        "\n",
        "            if not bloke:\n",
        "                if 条件をメタデーターとして追加する:\n",
        "                    pnginfo = PngImagePlugin.PngInfo()\n",
        "                    info = make_image.info\n",
        "                    for key, value in metadata.items():\n",
        "                        pnginfo.add_text(key, str(value))\n",
        "                    make_image.save(path_new, pnginfo=pnginfo)\n",
        "                else:\n",
        "                    make_image.save(path_new)\n",
        "                if 画像情報を表示:\n",
        "                    print(text_1.format(T+1),end=\"\")\n",
        "                    run_html_js(path_new,\"保存先のパス: \")\n",
        "                if 画像を表示:\n",
        "                    print()\n",
        "                    display(make_image)\n",
        "                if 画像情報を表示:\n",
        "                    print(text_2)\n",
        "            else:\n",
        "                print(\"ブロックされました\\n\")\n",
        "\n",
        "    def grid_task(self):\n",
        "        try:\n",
        "            y=y\n",
        "        except:\n",
        "            y=1\n",
        "        while True:\n",
        "            grid_test_path=os.path.join(self.Grid_save_path,f\"Grid_imgs-{y}.png\")\n",
        "            if os.path.exists(grid_test_path):\n",
        "                y+=1\n",
        "            else:\n",
        "                grid_path=grid_test_path\n",
        "                break\n",
        "        if num_imgs>1:\n",
        "            grid = image_grid(self.grid_imgs,cols=self.num_grid)\n",
        "            if grid is not None:\n",
        "                grid.save(grid_path)\n",
        "                grid_img = Image.open(grid_path)\n",
        "                if グリッド画像を表示:\n",
        "                    run_html_js(grid_path,\"グリッド画像のパス: \")\n",
        "                    #print(f\"\\033[34mgrid_save_path: \\033[32m{grid_path}\\033[0m\")\n",
        "                    display(grid_img)\n",
        "        print(\"\\033[38;2;135;206;235m1枚あたりの生成時間の平均: {:.2f}s\\033[0m\".format(self.generate_time_all_after))\n",
        "\n",
        "    def generate_video(self,seed,generator,image_save_path,sd_step,model_name,file_name,main_pipe):\n",
        "        T=1\n",
        "        chunk_size=4\n",
        "        path_new,file_new=path_token_video(self.Prompt_4,seed,model_name,guidance_scale,file_name,image_save_path)\n",
        "        now = datetime.datetime.now()\n",
        "        date_str = now.strftime(\"%Y-%m-%d_UTC-%H:%M:%S\")\n",
        "        path = os.path.join(image_save_path, file_name)\n",
        "        generate_start_time = time.time()\n",
        "        generator = torch.Generator(device)\n",
        "        generator.manual_seed(seed)\n",
        "        result = []\n",
        "        chunk_ids = np.arange(0, video_length, chunk_size - 1)\n",
        "        for T in range(len(chunk_ids)):\n",
        "            print(f\"生成プロセス {T + 1} / {len(chunk_ids)}\")\n",
        "            ch_start = chunk_ids[T]\n",
        "            ch_end = video_length if T == len(chunk_ids) - 1 else chunk_ids[T + 1]\n",
        "            frame_ids = [0] + list(range(ch_start, ch_end))\n",
        "            output = main_pipe(prompt=self.Prompt_4, video_length=len(frame_ids),num_inference_steps=sd_step,t0=(sd_step-5),t1=(sd_step-3) ,negative_prompt=negative_prompt2, guidance_scale=guidance_scale,height=height,width=width,generator=generator, frame_ids=frame_ids)\n",
        "            result.append(output.images[1:])\n",
        "        result = np.concatenate(result)\n",
        "        result = [(r * 255).astype(\"uint8\") for r in result]\n",
        "        imageio.mimsave(path_new, result, fps=int(video_fps))\n",
        "        generate_end_time=time.time()\n",
        "        generate_time=generate_end_time-generate_start_time\n",
        "        print(f\"\\033[34m動画生成が完了しました  {generate_time}s\")\n",
        "        if 画像情報を表示:\n",
        "            print(f\"\\033[34mseed値:\\033[32m {seed}\")\n",
        "            print(f\"\\033[34mファイルの名前:( \\033[32m{file_new})\\033[0m\")\n",
        "            run_html_js(path_new,\"動画ファイルのパス: \")\n",
        "        play_mp4(path_new)\n",
        "\n",
        "    def main_2(self,Prompt_list,make_images_list,seed,generator,num_grid,image_save_path,sd_step,model_name,file_name,main_pipe):\n",
        "        if pipeline_class_type==\"_txt2img\":\n",
        "            self.generate_images(Prompt_list,make_images_list,seed,generator,image_save_path,sd_step,model_name,file_name,main_pipe)\n",
        "        else:\n",
        "            if not self.img_dir:\n",
        "                self.img_path=self.img_path\n",
        "                self.init_image=img_set(self.img_path)\n",
        "                self.generate_images(Prompt_list,make_images_list,seed,generator,image_save_path,sd_step,model_name,file_name,main_pipe)\n",
        "            else:\n",
        "                for img_path in self.img_dict:\n",
        "\n",
        "                    self.init_image=img_set(img_path)\n",
        "                    self.generate_images(Prompt_list,make_images_list,seed,generator,image_save_path,sd_step,model_name,file_name,main_pipe)\n",
        "\n",
        "\n",
        "    def main_t(self,seed,generator,num_grid,image_save_path,sd_step,model_name,file_name,main_pipe):\n",
        "        if __name__ == '__main__':\n",
        "            self.GPU_P = threading.Thread(target=self.main_2, args=(Prompt_list,make_images_list,seed,generator,num_grid,image_save_path,sd_step,model_name,file_name,main_pipe))\n",
        "            self.GPU_last = threading.Thread(target=self.last_task,args=(make_images_list,))\n",
        "            self.GPU_P.start()\n",
        "            self.GPU_last.start()\n",
        "            self.GPU_last.join()\n",
        "        self.generate_time_all_after=self.generate_time_all/self.num_imgs\n",
        "        self.grid_task()\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "generate_cls=generate_class(main_pipe,Prompt_list,make_images_list,Grid_save_path,img_dir,img_path,img_dict,seed,generator,num_grid,Prompt_4,image_save_path,sd_step,model_path,file_name)\n",
        "if pipeline_class_type==\"_txt2video\":\n",
        "  generate_cls.generate_video(seed,generator,image_save_path,sd_step,model_path,file_name,main_pipe)\n",
        "else:\n",
        "  generate_cls.main_t(seed,generator,num_grid,image_save_path,sd_step,model_path,file_name,main_pipe)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#追加オプション"
      ],
      "metadata": {
        "id": "zi4EonYLXtN0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title #追加設定 {display-mode: \"form\"}\n",
        "\n",
        "# @markdown >スケジューラーがCustomEulerA or CustomEuler の場合のみ\n",
        "momentum = 1 # @param {type:\"slider\", min:0.8, max:1, step:0.05}\n",
        "momentum_hist = -0.1 # @param {type:\"slider\", min:-1, max:1, step:0.1}\n",
        "history_d = \"rand_init\" # @param [\"rand_new\", \"rand_init\"]\n",
        "\n",
        "\n",
        "# @markdown >img2img、Inpaintのみ\n",
        "strength = 1 # @param {type:\"slider\", min:0, max:1, step:0.1}\n",
        "入力する画像 = \"/content/IMG_2576.jpeg\" #@param {type:\"string\"}\n",
        "\n",
        "\n",
        "#@markdown >txt2video のみ\n",
        "video_length= 8  # @param {type:\"number\"}\n",
        "video_fps = 18 # @param {type:\"slider\", min:4, max:20, step:2}\n",
        "\n",
        "\n",
        "#@markdown >safeのみ\n",
        "\n",
        "safety_level = \"WEAK\" # @param [\"MAX\",\"STRONG\",\"MEDIUM\",\"WEAK\"]\n"
      ],
      "metadata": {
        "id": "5sEwtuowW9iP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import importlib\n",
        "import inspect\n",
        "\n",
        "\n",
        "def get_call_method(class_name:str,method_name:str='__call__') ->list:\n",
        "    call_method = getattr(getattr(diffusers, class_name),method_name)\n",
        "    parameters = inspect.signature(call_method).parameters\n",
        "    arg_names = []\n",
        "    for param in parameters.values():\n",
        "        arg_names.append(param.name)\n",
        "    return arg_names\n",
        "print(get_call_method(\"StableDiffusionPipeline\",\"__init__\"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zQGhCQJddHLi",
        "outputId": "94066a4e-ca27-4d1f-c263-e9eb7c1bf5b9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['self', 'vae', 'text_encoder', 'tokenizer', 'unet', 'scheduler', 'safety_checker', 'feature_extractor', 'image_encoder', 'requires_safety_checker']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import importlib\n",
        "import inspect\n",
        "\n",
        "module_name = \"diffusers\"\n",
        "class_name = \"StableDiffusionPipeline\"\n",
        "\n",
        "try:\n",
        "    module = importlib.import_module(module_name)\n",
        "    if hasattr(module, class_name):\n",
        "        class_object = getattr(module, class_name)\n",
        "        call_method = getattr(class_object, '__call__')\n",
        "        signature = inspect.signature(call_method)\n",
        "        parameters = signature.parameters\n",
        "\n",
        "        arg_names = []\n",
        "        for param in parameters.values():\n",
        "            if param.default != inspect.Parameter.empty:\n",
        "                arg_names.append(f\"{param.name}={param.default}\")\n",
        "            else:\n",
        "                arg_names.append(param.name)\n",
        "\n",
        "        print(f\"{class_name} の __call__ メソッドの引数リスト: {arg_names}\")\n",
        "    else:\n",
        "        print(f\"{class_name} クラスが見つかりません\")\n",
        "except ImportError:\n",
        "    print(f\"{module_name} モジュールが見つかりません\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kmCzGlfnIoOU",
        "outputId": "12d46206-3f1a-4c37-9188-5938969a686f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "StableDiffusionPipeline の __call__ メソッドの引数リスト: ['self', 'prompt=None', 'height=None', 'width=None', 'num_inference_steps=50', 'timesteps=None', 'guidance_scale=7.5', 'negative_prompt=None', 'num_images_per_prompt=1', 'eta=0.0', 'generator=None', 'latents=None', 'prompt_embeds=None', 'negative_prompt_embeds=None', 'ip_adapter_image=None', 'ip_adapter_image_embeds=None', 'output_type=pil', 'return_dict=True', 'cross_attention_kwargs=None', 'guidance_rescale=0.0', 'clip_skip=None', 'callback_on_step_end=None', \"callback_on_step_end_tensor_inputs=['latents']\", 'kwargs']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iIEW0jvHK7I1"
      },
      "source": [
        "#サブ機能"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cJ5y7hyEiCpa",
        "outputId": "c75cac45-908c-4440-e7bb-acaf34dd05cd"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "3701"
            ]
          },
          "execution_count": 41,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "#@title メモリの初期化(Beta) {display-mode: \"form\"}\n",
        "\n",
        "import gc\n",
        "import torch\n",
        "\n",
        "variable_names = [\"base_pipe\",\"txt2img_pipe\",\"img2img_pipe\",\"txt2video_pipe\",\"Inpaint_pipe\",\"safe_pipe\", \"tokenizer\"]\n",
        "\n",
        "for name in variable_names:\n",
        "    if name in globals():\n",
        "      del globals()[name]\n",
        "    if name in locals():\n",
        "      del locals()[name]\n",
        "\n",
        "torch.cuda.empty_cache()\n",
        "gc.collect()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2Hok9JfsGeQW"
      },
      "outputs": [],
      "source": [
        "#@title 画像のメタデータを見る  {display-mode: \"form\"}\n",
        "from PIL import Image, PngImagePlugin\n",
        "import os\n",
        "image_path = \"/content/Generated_images/Images/GIMG-68.png\" #@param {type:\"string\"}\n",
        "\n",
        "try:\n",
        "  output_info = Image.open(image_path).info\n",
        "except:\n",
        "  raise FileNotFoundError(\"画像を読み込めませんでした\")\n",
        "\n",
        "if image_path and os.path.isfile(image_path):\n",
        "    output_info = Image.open(image_path).info\n",
        "\n",
        "    for key,info in output_info.items():\n",
        "        print(f\"{key} : {info}\\n\")\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title txt2audio\n",
        "prompt = \"Techno-style  music\" #@param {type:\"string\"}\n",
        "negative_prompt = \"Low quality.\" #@param {type:\"string\"}\n",
        "audio_length_in_s = 30 #@param {type:\"number\"}\n",
        "num_inference_steps = 500 #@param {type:\"number\"}\n",
        "seed = -1 #@param {type:\"number\"}\n",
        "\n",
        "prompt+=\",masterpiece\"\n",
        "import os\n",
        "import scipy\n",
        "import torch\n",
        "from diffusers import AudioLDM2Pipeline\n",
        "from IPython.display import Audio\n",
        "import random\n",
        "\n",
        "\n",
        "\n",
        "repo_id = \"cvssp/audioldm2\"\n",
        "if \"pipe\" not in locals() and \"pipe\" not in globals():\n",
        "    pipe = AudioLDM2Pipeline.from_pretrained(repo_id, torch_dtype=torch.float16).to(\"cuda\")\n",
        "if \"numa\" in globals():\n",
        "    numa+=1\n",
        "else:\n",
        "    numa=1\n",
        "if seed is None or seed==-1:\n",
        "    seed = random.randint(1,1000000)\n",
        "generator = torch.Generator(\"cuda\").manual_seed(seed)\n",
        "audio = pipe(prompt,\n",
        "             negative_prompt=negative_prompt,\n",
        "             num_inference_steps=num_inference_steps,\n",
        "             audio_length_in_s=audio_length_in_s,\n",
        "             num_waveforms_per_prompt=1,\n",
        "             generator=generator,\n",
        "             ).audios\n",
        "\n",
        "\n",
        "os.makedirs(\"/content/audio\",exist_ok=True)\n",
        "save_path=f\"/content/audio/audio_{numa}_{seed}.wav\"\n",
        "\n",
        "\n",
        "scipy.io.wavfile.write(save_path, rate=16000, data=audio[0])\n",
        "print(f\"\\033[38;2;0;255;255mseed: {seed}\")\n",
        "print(f\"save_path: {save_path}\\n\\033[0m\")\n",
        "Audio(save_path)\n"
      ],
      "metadata": {
        "id": "i6YlYLKMSU6K",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KRB6MLjGbrof"
      },
      "outputs": [],
      "source": [
        "#@title Youtube_download {display-mode: \"form\"}\n",
        "import os\n",
        "from google.colab import files\n",
        "import shutil\n",
        "\n",
        "try:\n",
        "    import pytube\n",
        "except ModuleNotFoundError:\n",
        "    !pip install -q pytube\n",
        "\n",
        "from pytube import (Playlist, YouTube)\n",
        "from requests import HTTPError\n",
        "Youtube_path = \"\" # @param {type:\"string\"}\n",
        "\n",
        "ローカルにダウンロードする = False # @param {type:\"boolean\"}\n",
        "def Youtube_download(Youtube_path,local_download):\n",
        "    Not_download_list=[]\n",
        "    mkdir_list=[\"/content/Youtube/File\",\"/content/Youtube/List\"]\n",
        "    for mk_path in mkdir_list:\n",
        "        os.makedirs(mk_path,exist_ok=True)\n",
        "    os.chdir(\"/content/Youtube\")\n",
        "    if Youtube_path:\n",
        "        if Youtube_path.startswith(\"https://www.youtube.com/playlist?list=\"):\n",
        "            try:\n",
        "                nu+=1\n",
        "            except:\n",
        "                nu=1\n",
        "            list_path=f\"/content/Youtube/Yotube_list_{nu}\"\n",
        "            os.makedirs(list_path,exist_ok=True)\n",
        "            os.chdir(list_path)\n",
        "            Y_list = Playlist(Youtube_path)\n",
        "            for video, path in zip(Y_list.videos, Y_list.video_urls[:3]):\n",
        "                try:\n",
        "                    video.streams.first().download()\n",
        "                except:\n",
        "                    Not_download_list.append(path)\n",
        "            if Not_download_list:\n",
        "                print(\"ダウンロードに失敗したパス\")\n",
        "                for ndl in Not_download_list:\n",
        "                    print(ndl)\n",
        "            if local_download:\n",
        "                zip_path=f\"/content/Youtube/zip-No_{zip_number}.zip\"\n",
        "                shutil.make_archive(zip_path, \"zip\", list_path)\n",
        "                files.download(zip_path)\n",
        "\n",
        "        elif Youtube_path.startswith(\"https://www.youtube.com/watch?v=\"):\n",
        "            os.chdir(\"/content/Youtube/File\")\n",
        "            try:\n",
        "                yt=YouTube(Youtube_path)\n",
        "                yt.streams.first().download()\n",
        "                yt_save_path=os.path.join(\"/content/Youtube/File\" , (yt.title+\".mp4\"))\n",
        "            except:\n",
        "                raise HTTPError(\"URLが無効です\")\n",
        "            files.download(yt_save_path)\n",
        "        else:\n",
        "            raise HTTPError(\"URLが無効です\")\n",
        "    else:\n",
        "        raise HTTPError(\"URLの入力をお願いします\")\n",
        "\n",
        "    os.chdir(\"/content\")\n",
        "    print(\"処理が終了しました\")\n",
        "\n",
        "Youtube_download(Youtube_path,ローカルにダウンロードする)\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FC3E3W81SL8n"
      },
      "outputs": [],
      "source": [
        "#@title  グリッド画像の分割  (iamge generation){display-mode: \"form\"}\n",
        "from PIL import Image\n",
        "import os\n",
        "import sys\n",
        "import math\n",
        "\n",
        "### 各定数の設定\n",
        "# 縦方向分割数\n",
        "\n",
        "# 分割元ファイルパス\n",
        "input_path = \"\" # @param {type:\"string\"}\n",
        "output_dir = \"\" # @param {type:\"string\"}\n",
        "縦の分割回数 = 1 # @param {type:\"number\"}\n",
        "横の分割回数 = 3 # @param {type:\"number\"}\n",
        "#@markdown >heightとwidthは2以上の自然数の入力をお願いします\n",
        "height=縦の分割回数\n",
        "width=横の分割回数\n",
        "base_img_path=input_path\n",
        "\n",
        "\n",
        "if base_img_path==\"\" or  output_dir == \"\":\n",
        "  raise TypeError(\"pathが未入力です\")\n",
        "if output_dir is not dir:\n",
        "   output_dir = os.path.dirname(output_dir)\n",
        "\n",
        "def is_positive_integer(value):\n",
        "    try:\n",
        "        value = int(value)  # 整数に変換\n",
        "        if value > 0:  # 1以上の場合は True を返す\n",
        "            return True\n",
        "        else:\n",
        "            return False\n",
        "    except ValueError:  # ValueError が発生した場合は False を返す\n",
        "        return False\n",
        "\n",
        "if not is_positive_integer(height) or is_positive_integer(width):\n",
        "  raise TypeError(\"heightとwidthは、2以上の自然数である必要があります。\")\n",
        "\n",
        "def ImgSplit(im,w,h):\n",
        "    # 読み込んだ画像の高さと幅を指定分割数で割る\n",
        "    HEIGHT = h / height\n",
        "    WIDTH = w / width\n",
        "\n",
        "    # 縦の分割枚数\n",
        "    for h1 in range(height):\n",
        "        # 横の分割枚数\n",
        "        for w1 in range(width):\n",
        "            w2 = w1 * WIDTH\n",
        "            h2 = h1 * HEIGHT\n",
        "            yield im.crop((w2, h2, WIDTH + w2, HEIGHT + h2))\n",
        "\n",
        "#if __name__ == '__main__': このセルが直接実行された場合\n",
        "    # 画像の読み込み\n",
        "im = Image.open(base_img_path)\n",
        "w = im.size[0]\n",
        "h = im.size[1]\n",
        "length = math.log10(height * width) + 1\n",
        "os.makedirs(output_dir, exist_ok=True)\n",
        "for number, ig in enumerate(ImgSplit(im,w,h), 1):\n",
        "    # 出力\n",
        "    ig.save(output_dir + \"/\" + str(number).zfill(int(length)) + \".PNG\", \"PNG\")\n",
        "print(f\"\\033[34m分割が完了しました: {output_dir}\\033[0m\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3al9BGGP7DIR"
      },
      "outputs": [],
      "source": [
        "#@title  単語が含まれているファイルを移動{display-mode: \"form\"}\n",
        "import os\n",
        "import shutil\n",
        "\n",
        "def move_files_with_keyword(source_dir, destination_dir, keyword):\n",
        "    # 指定されたディレクトリ内のファイルを取得\n",
        "    files = os.listdir(source_dir)\n",
        "\n",
        "    for file in files:\n",
        "        # ファイル名にキーワードが含まれているかチェック\n",
        "        if keyword in file:\n",
        "            # 移動元のファイルパス\n",
        "            source_file = os.path.join(source_dir, file)\n",
        "            # 移動先のファイルパス\n",
        "            destination_file = os.path.join(destination_dir, file)\n",
        "\n",
        "            # ファイルを移動\n",
        "            shutil.move(source_file, destination_file)\n",
        "\n",
        "# 使用例\n",
        "検索対象のフォルダ = \"/content/drive/MyDrive/GS\"  # @param {type:\"string\"}\n",
        "移動先 = \"/content/drive/MyDrive/GS_grid\"   # @param {type:\"string\"}\n",
        "単語 = \"grid_imgs\" # @param {type:\"string\"}\n",
        "\n",
        "source_directory=検索対象のフォルダ\n",
        "destination_directory=移動先\n",
        "keyword_to_search=単語\n",
        "os.makedirs(destination_directory)\n",
        "move_files_with_keyword(source_directory, destination_directory, keyword_to_search)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "QH5cPNi297Lr",
        "outputId": "7ffffedb-68d4-43a9-f11b-b5e1038a6e3b"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_f0aab976-76ba-437f-b7ff-12825823063c\", \"GIMG-68.png\", 1040758)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[32mダウンロードが正常に完了しました\n"
          ]
        }
      ],
      "source": [
        "#@title  ダウンロード{display-mode: \"form\"}\n",
        "#@markdown >ファイル or ディレクトリを指定してダウンロードします。\n",
        "\n",
        "\n",
        "from google.colab import files\n",
        "import shutil\n",
        "import os\n",
        "path = \"/content/Generated_images/Images/GIMG-68.png\" #@param {type:\"string\"}\n",
        "\n",
        "# 最後が特定の単語であるかをチェックする\n",
        "if path.endswith(\".png\" or \".jpg\"):\n",
        "  try:\n",
        "    files.download(path)\n",
        "    print(\"\\033[32mダウンロードが正常に完了しました\")\n",
        "  except:\n",
        "    print(\"\\033[31m指定された画像が見つかりませんでした\\033[0m\")\n",
        "elif path.endswith(\".mp4\"):\n",
        "  try:\n",
        "    files.download(path)\n",
        "    print(\"\\033[32mダウンロードが正常に完了しました\")\n",
        "  except:\n",
        "    print(\"\\033[31m指定された動画ファイルが見つかりませんでした\\033[0m\")\n",
        "else:\n",
        "  try:\n",
        "    zip_number+=1\n",
        "  except:\n",
        "    zip_number=1\n",
        "  zip_name=f\"Generated_images-No.{zip_number}\"\n",
        "  zip_save_dir=os.path.join(\"/content\",zip_name)\n",
        "  zippath=zip_save_dir+\".zip\"\n",
        "  try:\n",
        "    shutil.make_archive(zip_save_dir, \"zip\", path)\n",
        "    files.download(zippath)\n",
        "    print(\"\\033[32mzipファイルへ圧縮が正常に完了しました\")\n",
        "    print(f'zipファイル名前は\\033[34m\"{zip_name}\"\\033[32mですご確認ください\\033[0m')\n",
        "  except:\n",
        "    zip_number-=1\n",
        "    print(\"\\033[31m指定されたディレクトリが見つかりませんでした\\033[0m\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KuI2OK7YI2pE"
      },
      "outputs": [],
      "source": [
        "#@title dir_size_counter {display-mode: \"form\"}\n",
        "#@markdown\n",
        "import os\n",
        "\n",
        "def get_folder_size(folder_path):\n",
        "    total_size = 0\n",
        "    if os.path.isfile(folder_path):\n",
        "        total_size+=os.path.getsize(folder_path)\n",
        "    else:\n",
        "        for path, dirs, files in os.walk(folder_path):\n",
        "            for file in files:\n",
        "                file_path = os.path.join(path, file)\n",
        "                total_size += os.path.getsize(file_path)\n",
        "\n",
        "    return total_size\n",
        "\n",
        "folder_path = \"\" # @param {type:\"string\"}\n",
        "if not os.path.exists(folder_path):\n",
        "    raise FileNotFoundError(\"pathが無効です\")\n",
        "\n",
        "size_in_bytes = get_folder_size(folder_path)\n",
        "\n",
        "\n",
        "size_in_kb = size_in_bytes / 1024\n",
        "size_in_mb = size_in_kb / 1024\n",
        "size_in_gb = size_in_mb / 1024\n",
        "\n",
        "print(f\"Folder Size: {size_in_bytes} bytes\")\n",
        "print(f\"Folder Size: {size_in_kb:.2f} KB\")\n",
        "print(f\"Folder Size: {size_in_mb:.2f} MB\")\n",
        "print(f\"Folder Size: {size_in_gb:.2f} GB\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BRRD_VF59fA_"
      },
      "outputs": [],
      "source": [
        "#@title 画像変換 {display-mode: \"form\"}\n",
        "import os\n",
        "from PIL import Image\n",
        "import sys\n",
        "input_path = \"\" #@param {type:\"string\"}\n",
        "output_path=\"\" #@param {type:\"string\"}\n",
        "Change_mode = \"png2jpg\" # @param [\"png2jpg\", \"jpg2png\"]\n",
        "\n",
        "#@markdown * png2jpg :png画像をjpg画像に変換します。\n",
        "\n",
        "\n",
        "#@markdown * jpg2png :jpg画像をpng画像に変換します。\n",
        "\n",
        "if not os.path.exists(input_path):\n",
        "  #raise FileNotFoundError('\\033[33m指定された \"dir\"または\"file\" が見つかりませんでした。\\033[0m')\n",
        "  sys.exit('\"input_path\"にdirまたはfileのパスを入力お願いします。')\n",
        "\n",
        "if not input_path.endswith(\".png\" or \".jpg\" or \".jpeg\"):\n",
        "  if output_path==\"\":\n",
        "    # 保存先のディレクトリパスを作成\n",
        "    save_dir_path = input_path +\"-\" +Change_mode\n",
        "  else:\n",
        "    save_dir_path=output_path\n",
        "  # 保存先のディレクトリが存在しない場合は作成する\n",
        "  if not os.path.exists(save_dir_path):\n",
        "    os.makedirs(save_dir_path)\n",
        "\n",
        "  # ディレクトリ内のファイルを取得\n",
        "  file_list = os.listdir(input_path)\n",
        "\n",
        "  # ファイルごとに処理を行う\n",
        "  for file_name in file_list:\n",
        "    # ファイルのパスを作成\n",
        "    file_path = os.path.join(save_dir_path, file_name)\n",
        "\n",
        "    # ファイルの拡張子を取得\n",
        "    file_extension = os.path.splitext(file_name)[1]\n",
        "    if Change_mode == \"png2jpg\":\n",
        "    # 最後が特定の単語であるかをチェックする\n",
        "      if file_extension.endswith(\".png\"):\n",
        "        try:\n",
        "          # 画像を開く\n",
        "          im = Image.open(file_path)\n",
        "          file_name_new = file_name.replace(\".png\", \".jpg\")#pngをjpg\n",
        "          # 保存先のファイルパスを作成\n",
        "          save_file_path = os.path.join(save_dir_path, file_name_new)\n",
        "          # 変換後の画像を保存する\n",
        "          im.save(save_file_path,format=\"JPEG\")\n",
        "          print(f\"\\033[34m{save_file_path}を保存しました。\\033[0m\")\n",
        "        except Exception as e:\n",
        "          print(f\"\\033[31m{file_name}の変換中にエラーが発生しました: {str(e)}\\033[0m\")\n",
        "      else:\n",
        "        print(f\"{file_name}はpngファイルではないため、変換をスキップしました。\")\n",
        "    elif Change_mode == \"jpg2png\":\n",
        "        try:\n",
        "          # 画像を開く\n",
        "          im = Image.open(file_path)\n",
        "          file_name_new = file_name.replace((\".jpg\"or\".jpeg\"), \".png\")#jpgをpng\n",
        "          # 保存先のファイルパスを作成\n",
        "          save_file_path = os.path.join(save_dir_path, file_name_new)\n",
        "          # 変換後の画像を保存する\n",
        "          im.save(save_file_path,format=\"PNG\")\n",
        "          print(f\"\\033[34m{save_file_path}を保存しました。\\033[0m\")\n",
        "        except Exception as e:\n",
        "          print(f\"\\033[31m{file_path}の変換中にエラーが発生しました: {str(e)}\\033[0m\")\n",
        "else:\n",
        "  if Change_mode ==\"png2jpg\":\n",
        "    if not output_path==\"\":\n",
        "      basename_new = input_path.replace(\".png\", \".jpg\")#jpgをpng\n",
        "      save_file_path=os.path.join(output_path, basename_new)\n",
        "      im = Image.open(input_path)\n",
        "      im.save(save_file_path,format=\"JPEG\")\n",
        "    else:\n",
        "      im = Image.open(input_path)\n",
        "      save_file_path = input_path.replace(\".png\", \".jpg\")\n",
        "      im.save(save_file_path,format=\"JPEG\")\n",
        "    print(f'\\033[34m\"{save_file_path}\"に保存しました。\\033[0m')\n",
        "  elif Change_mode == \"jpg2png\":\n",
        "    if not output_path==\"\":\n",
        "      basename_new = input_path.replace((\".jpg\"or\".jpeg\"), \".png\")#jpgをpng\n",
        "      save_file_path=os.path.join(output_path, basename_new)\n",
        "      im = Image.open(input_path)\n",
        "      im.save(save_file_path,format=\"PNG\")\n",
        "    else:\n",
        "      im = Image.open(input_path)\n",
        "      save_file_path = input_path.replace((\".jpg\"or\".jpeg\"), \".png\")\n",
        "      im.save(save_file_path,format=\"PNG\")\n",
        "    print(f'\\033[34m\"{save_file_path}\"に保存しました。\\033[0m')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YyeuVK1DeJEv"
      },
      "outputs": [],
      "source": [
        "#@title mp4を再生  {display-mode: \"form\"}\n",
        "MP4_path= \"\" #@param {type:\"string\"}\n",
        "# mp4動画の再生\n",
        "import os\n",
        "from IPython.display import HTML\n",
        "from base64 import b64encode\n",
        "try:\n",
        "  mp4 = open(MP4_path, 'rb').read()\n",
        "except:\n",
        "  raise FileNotFoundError(\"指定されたmp4が見つかりませんでした\")\n",
        "data_url = 'data:video/mp4;base64,' + b64encode(mp4).decode()\n",
        "HTML(f\"\"\"\n",
        "<video width=\"100%\" height=\"100%\" controls>\n",
        "      <source src=\"{data_url}\" type=\"video/mp4\">\n",
        "</video>\"\"\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yT_pKvlmJXIq"
      },
      "outputs": [],
      "source": [
        "#@title  文字列トークンカウンター{display-mode: \"form\"}\n",
        "import pprint\n",
        "from transformers import AutoTokenizer\n",
        "\n",
        "prompt = \"\"  #@param {type:\"string\"}\n",
        "try:\n",
        "  tokens = tokenizer.tokenize(prompt)\n",
        "except:\n",
        "  text_model_id = \"laion/CLIP-ViT-H-14-laion2B-s32B-b79K\"\n",
        "  tokenizer = AutoTokenizer.from_pretrained(text_model_id)\n",
        "  tokens = tokenizer.tokenize(prompt)\n",
        "print(len(tokens))\n",
        "pprint.pprint(tokens)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "J-5Gh7XWLwcr"
      },
      "outputs": [],
      "source": [
        "#@title 高画質化  {display-mode: \"form\"}\n",
        "from PIL import Image\n",
        "from io import BytesIO\n",
        "from diffusers import StableDiffusionUpscalePipeline\n",
        "import torch\n",
        "\n",
        "# load model and scheduler\n",
        "Prompt = \"\" #@param {type:\"string\"}\n",
        "prompt=\"masterpiece:2.0,best quality,high quality,\"+Prompt\n",
        "low_res_img_path = \"/content/IMG_1708.jpeg\" #@param {type:\"string\"}\n",
        "encoded_text = codecs.encode(low_res_img_path, 'utf-8')\n",
        "low_res_img_path = codecs.decode(encoded_text, 'utf-8')\n",
        "\n",
        "if \"pipeline2\" not in locals()  and \"pipeline2\" not in globals():\n",
        "  model_id = \"stabilityai/stable-diffusion-x4-upscaler\"\n",
        "  pipeline2 = StableDiffusionUpscalePipeline.from_pretrained(\n",
        "    model_id, revision=\"fp16\", torch_dtype=torch.float16\n",
        "  )\n",
        "  pipeline2 = pipeline2.to(\"cuda\")\n",
        "if not os.path.exists(low_res_img_path):\n",
        "  raise FileNotFoundError(\"ファイルが見つかりませんでした\")\n",
        "try:\n",
        "  low_res_img = Image.open(low_res_img_path)\n",
        "except:\n",
        "  raise FileNotFoundError(\"画像を読み込めませんでした\")\n",
        "low_res_img = low_res_img.resize((128, 128))\n",
        "\n",
        "negative_prompt=\"low quality:2.0\"\n",
        "\n",
        "upscaled_image = pipeline2(prompt=prompt, image=low_res_img,negative_prompt=negative_prompt).images[0]\n",
        "\n",
        "try:\n",
        "  L+=1\n",
        "except:\n",
        "  L=1\n",
        "os.makedirs(\"/content/low_res_imgs\",exist_ok=True)\n",
        "path1=(f\"/content/low_res_imgs/No{L}.png\")\n",
        "upscaled_image.save(path1)\n",
        "print(f\"画像保存パス: ({path1})\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W4kpwhy30ePA"
      },
      "outputs": [],
      "source": [
        "#@title 画像フォルダを削除  {display-mode: \"form\"}\n",
        "\n",
        "del_path = \"/content/drive/MyDrive/korect/Images\" #@param {type:\"string\"}\n",
        "if del_path ==\"\":\n",
        "  del_path=\"/content/Generated_images\"\n",
        "import shutil\n",
        "if not os.path.isdir(del_path):\n",
        "  raise TypeError(\"ディレクトリのみ削除が可能です\")\n",
        "#@markdown 未入力の場合 /content/Generated_imagesを削除します\n",
        "\n",
        "YorS=input(\"本当に削除しますか？[yes/no]: \")\n",
        "if YorS.lower() in (\"yes\", \"y\"): # 入力された文字列を小文字にして、yesやyと一致するか判定する\n",
        "  try:\n",
        "    shutil.rmtree(del_path)\n",
        "    print(\"削除しました\")\n",
        "  except FileNotFoundError:\n",
        "    print(f\"\\033[31m{del_path}が見つかりませんでした\\033[0m\")\n",
        "elif YorS.lower() in (\"no\", \"n\"): # 入力された文字列を小文字にして、noやnと一致するか判定する\n",
        "  print(\"フォルダの削除を中止しました\")\n",
        "else:\n",
        "  print(\"yes/no のみの入力をお願いします。\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "t-Qqqp6la4WJ"
      },
      "outputs": [],
      "source": [
        "#@title url_Download\n",
        "import urllib.request , os\n",
        "import datetime\n",
        "def download_file(url, save_path):\n",
        "  other_url,split=os.path.splitext(url)\n",
        "  if save_path==\"\":\n",
        "    save_path=\"/content/download\"\n",
        "    choice_name=input(\"ファイル名(拡張子無し): \")\n",
        "    choice_name+=split\n",
        "    test_path=os.path.join(save_path,choice_name)\n",
        "    if os.path.exists(test_path):\n",
        "      now=datetime.now()\n",
        "      save_path=test_path+now\n",
        "  save_dir=os.path.dirname(save_path)\n",
        "  os.makedirs(save_dir,exist_ok=True)\n",
        "  if not url.endswith(split):\n",
        "    save_path+=split\n",
        "  try:\n",
        "    urllib.request.urlretrieve(url, save_path)\n",
        "  except HTTPError:\n",
        "    raise SystemError(\"URLからファイルを習得できませんでした\")\n",
        "  print(f\"保存先のパス: {save_path}\")\n",
        "\n",
        "url = \"\" #@param {type:\"string\"}\n",
        "save_path = \"\" #@param {type:\"string\"}\n",
        "\n",
        "download_file(url, save_path)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@markdown >そのうち使うかもしれないclass\n",
        "class fomat_class:\n",
        "    def to_list(self,string):\n",
        "        return_list=[]\n",
        "        if type(string) is str:\n",
        "            return_list.append(string)\n",
        "            return return_list\n",
        "        else:\n",
        "            return list(string)\n",
        "\n",
        "\n",
        "    def input_fomat(self,Value,zero_txt=\"\", return_txt=\"\",print_txt=\"パス\",auto=False):\n",
        "        if zero_txt and return_txt:\n",
        "            special_state=True\n",
        "            num=0\n",
        "        else:\n",
        "            special_state=False\n",
        "            num=1\n",
        "\n",
        "        if type(Value) is dict:\n",
        "            Value_is_dict=True\n",
        "            first_value_length = len(list(Value.values())[0])\n",
        "            for value in my_dict.values():\n",
        "                if len(value) != first_value_length:\n",
        "                   raise TypeError(\"The number of elements per key in the dictionary must be uniform\")\n",
        "        else:\n",
        "            Value_is_dict=False\n",
        "        if auto:\n",
        "            if Value_is_dict:\n",
        "                return list(Value.get(list(Value.keys())[0], []))\n",
        "            else:\n",
        "                choice_path=Value[0]\n",
        "                return self.to_list(choice_path)\n",
        "\n",
        "        if len(Value)>=15:\n",
        "            start_number=\"1\"\n",
        "            if special_state:\n",
        "                start_number=\"0\"\n",
        "                print(f\"\\033[34m0.{zero_txt}\")\n",
        "\n",
        "            if Value_is_dict:\n",
        "                sorted_with_like = sorted(Value.items(), key=lambda x: x[1][1], reverse=True)\n",
        "                for i, (model_id, (model_name, like)) in enumerate(sorted_with_like, 1):\n",
        "                    if i>15:\n",
        "                        break\n",
        "                    print(f\"\\033[34m{i}.{print_txt}: {model_name}, 評価: {like}\")\n",
        "            else:\n",
        "                for i in range(1,16):#15個\n",
        "                    print(f\"\\033[34m{i+1}.{print_txt}: {Value[i]}\\033[0m\")\n",
        "\n",
        "\n",
        "            print(f\"\\033[34m16.上記の{print_txt}以外 (全ての候補を表示します)\\033[0m\\n\")\n",
        "            while True:\n",
        "                choice = input(f\"{print_txt}の選択({start_number}~16): \")\n",
        "                try:\n",
        "                    choice=int(choice)\n",
        "                except:\n",
        "                    print(\"\\033[33m自然数のみ有効です\\033[34m\")\n",
        "                    continue\n",
        "                if special_state and choice==0:\n",
        "                    print(\"\\033[0m\",end=\"\")\n",
        "                    return self.to_list(return_txt)\n",
        "\n",
        "                if choice==16: #other_file\n",
        "                    break\n",
        "                elif 1<=choice<=15:\n",
        "                    if Value_is_dict:\n",
        "                        choice_path = Value.get(list(Value.keys())[choice - 1], [])\n",
        "                    else:\n",
        "                        choice_path=Value[choice-1]\n",
        "                    print(\"\\033[0m\",end=\"\")\n",
        "                    return self.to_list(choice_path)\n",
        "                else:\n",
        "                    print(f\"\\033[33m{num}~16 までの数字の入力をお願いします\\033[34m\")\n",
        "\n",
        "        start_number=\"1\"\n",
        "        if special_state:\n",
        "            start_number=\"0\"\n",
        "            print(f\"\\033[34m0.{zero_txt}\")\n",
        "        for i, file_name in enumerate(Value, 1):\n",
        "            print(f\"\\033[34m{i}.{print_txt}: {file_name}\")\n",
        "        while True:\n",
        "            choice = input(f\"使用する{print_txt}の選択({start_number}~{len(Value)}): \")\n",
        "            try:\n",
        "                choice=int(choice)\n",
        "            except:\n",
        "                print(\"\\033[33m自然数のみ有効です\\033[34m\")\n",
        "                continue\n",
        "            print(\"\\033[0m\",end=\"\")\n",
        "            if special_state and choice==0:\n",
        "                return self.to_list(return_txt)\n",
        "            elif 1<=choice<=len(Value):\n",
        "                if Value_is_dict:\n",
        "                    choice_path = Value.get(list(Value.keys())[choice - 1], [])\n",
        "                    return self.to_list(choice_path)\n",
        "                else:\n",
        "                    choice_path=Value[choice-1]\n",
        "                    return self.to_list(choice_path)\n",
        "            else:\n",
        "                print(f\"\\033[33m{num}~{len(Value)} までの数字の入力をお願いします\\033[34m\")\n",
        "format=format_class"
      ],
      "metadata": {
        "id": "0rev0k4HgF5l",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a1QPxPSzF_aP"
      },
      "source": [
        "#学習"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HH1aO3M-zk5x"
      },
      "outputs": [],
      "source": [
        "#@title モデルを学習 {display-mode: \"form\"}\n",
        "\n",
        "学習で使用する単語 = \"Cat ears girl\" # @param {type:\"string\"}\n",
        "#概念を説明する単語 = \"\" # @param {type:\"string\"}\n",
        "学習ステップ = 20   # @param {type:\"number\"}\n",
        "\n",
        "入力する画像フォルダ = \"/content/drive/MyDrive/GS\" # @param {type:\"string\"}\n",
        "出力するフォルダ = \"/content/drive/MyDrive/make_model_2\" # @param {type:\"string\"}\n",
        "#概念のタイプ = \"style\" # @param [\"style\",\"object\"]\n",
        "モデル名前_or_パス = \"/content/drive/MyDrive/loliDiffusion_SFW_.safetensors\" # @param [\"runwayml/stable-diffusion-v1-5\", \"stabilityai/stable-diffusion-2-1\"] {allow-input: true}\n",
        "#出力ファイル名 = \"\" # @param {type:\"string\"}\n",
        "\n",
        "\n",
        "#@markdown ---\n",
        "\n",
        "終了したらランタイムを切断 = False # @param {type:\"boolean\"}\n",
        "#@markdown sd1.5系列とsd2.1系列では埋め込みに互換性がありません。\n",
        "\n",
        "driveに接続 = True # @param {type:\"boolean\"}\n",
        "\n",
        "# @markdown >用語の説明\n",
        "# @markdown * placeholder_token : 学習内で、使用するトークンです。新しい概念を入力します。\n",
        "# @markdown * initializer_token : 新しい概念が何であるかを要約する単語です。\n",
        "import os\n",
        "from google.colab import drive\n",
        "drive_cn=driveに接続\n",
        "if driveに接続:\n",
        "  if not drive._os.path.ismount('/content/drive'):\n",
        "    try:\n",
        "      drive.mount('/content/drive')\n",
        "    except:\n",
        "      print(\"ドライブの接続に失敗しました。\")\n",
        "      drive_cn=False\n",
        "\n",
        "\n",
        "\n",
        "if  drive_cn==False and \"/content/drive/MyDrive\" in 出力するフォルダ:\n",
        "  raise TypeError(\"Googleドライブに接続されていないため、ドライブに保存できません。\")\n",
        "\n",
        "if not os.path.isdir(入力する画像フォルダ):\n",
        "  raise FileNotFoundError(\"画像フォルダが存在しません\")\n",
        "\n",
        "if 出力するフォルダ is None:\n",
        "  raise FileNotFoundError(\"結果を出力するフォルダを入力してください\")\n",
        "\n",
        "\n",
        "\n",
        "os.makedirs(出力するフォルダ,exist_ok=True)\n",
        "try:\n",
        "  import diffusers,transformers\n",
        "except:\n",
        "  !pip install transformers diffusers -q\n",
        "  import transformers,diffusers\n",
        "\n",
        "if not os.path.exists(\"/content/script/diffusers\"):\n",
        "  %cd /content/script\n",
        "  !git clone https://github.com/huggingface/diffusers\n",
        "  %cd diffusers\n",
        "\n",
        "try:\n",
        "  import accelerate,transformers,ftfy\n",
        "except:\n",
        "  !pip install -q accelerate>=0.16.0 transformers>=4.25.1 ftfy Jinja2 bitsandbytes\n",
        "  import bitsandbytes,accelerate,transformers,ftfy\n",
        "import accelerate,torchvision,transformers,ftfy ,diffusers,bitsandbytes\n",
        "\n",
        "size=512\n",
        "if モデル名前_or_パス==\"runwayml/stable-diffusion-v1-5\":\n",
        "  size=512\n",
        "#else:\n",
        "#  size=768\n",
        "\n",
        "#exportは使用不可\n",
        "%env pretrained_model_name_or_path=$モデル名前_or_パス\n",
        "%env train_data_dir=$入力する画像フォルダ\n",
        "%env placeholder_token=$学習で使用する単語\n",
        "%env resolution=$size\n",
        "%env max_train_steps=$学習ステップ\n",
        "%env output_dir=$出力するフォルダ\n",
        "\n",
        "\n",
        "%cd /content/script/diffusers/examples/textual_inversion\n",
        "\n",
        "!accelerate launch train_dreambooth.py \\\n",
        "  --pretrained_model_name_or_path=$pretrained_model_name_or_path  \\\n",
        "  --instance_data_dir=$train_data_dir \\\n",
        "  --output_dir=$output_dir \\\n",
        "  --instance_prompt=$placeholder_token \\\n",
        "  --resolution=$resolution \\\n",
        "  --train_batch_size=1 \\\n",
        "  --learning_rate=1 \\\n",
        "  --lr_scheduler=\"constant\" \\\n",
        "  --lr_warmup_steps=0 \\\n",
        "  --max_train_steps=$max_train_steps \\\n",
        "\n",
        "base=os.path.join(出力するフォルダ,\"learned_embeds.safetensors\")\n",
        "if not 出力ファイル名==\"\":\n",
        "  after_name=出力ファイル名+\".safetensors\"\n",
        "  after=os.path.join(出力するフォルダ,after_name)\n",
        "  if os.path.exists(base):\n",
        "    os.rename(base,after)\n",
        "  else:\n",
        "    print(\"埋め込みファイルが見つかりませんでした。\")\n",
        "else:\n",
        "  after=base\n",
        "\n",
        "print(f\"\"\"\\033[34m\n",
        "学習が終了しました\n",
        "path: {出力するフォルダ}\\033[0m\n",
        "\"\"\")\n",
        "\n",
        "if 終了したらランタイムを切断:\n",
        "  from google.colab import runtime\n",
        "  print(\"ランタイムを切断中...\")\n",
        "  runtime.unassign()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "R-PpdeFgFjwa"
      },
      "outputs": [],
      "source": [
        "#@title 埋め込みを学習 {display-mode: \"form\"}\n",
        "\n",
        "学習で使用するトークン = \"flans\" # @param {type:\"string\"}\n",
        "概念を説明する単語 = \"flandre\" # @param {type:\"string\"}\n",
        "学習ステップ = 50   # @param {type:\"number\"}\n",
        "\n",
        "入力する画像フォルダ = \"/content/drive/MyDrive/GS\" # @param {type:\"string\"}\n",
        "出力するフォルダ = \"/content/drive/MyDrive/textjual\" # @param {type:\"string\"}\n",
        "概念のタイプ = \"style\" # @param [\"style\",\"object\"]\n",
        "モデル名前_or_パス = \"runwayml/stable-diffusion-v1-5\" # @param [\"runwayml/stable-diffusion-v1-5\", \"stabilityai/stable-diffusion-2-1\"] {allow-input: true}\n",
        "\n",
        "出力ファイル名 = \"Flans\" # @param {type:\"string\"}\n",
        "\n",
        "\n",
        "#@markdown ---\n",
        "\n",
        "終了したらランタイムを切断 = False # @param {type:\"boolean\"}\n",
        "#@markdown sd1.5系列とsd2.1系列では埋め込みに互換性がありません。\n",
        "\n",
        "driveに接続 = True # @param {type:\"boolean\"}\n",
        "\n",
        "# @markdown >用語の説明\n",
        "# @markdown * placeholder_token : 学習内で、使用するトークンです。新しい概念を入力します。\n",
        "# @markdown * initializer_token : 新しい概念が何であるかを要約する単語です。\n",
        "import os\n",
        "from google.colab import drive\n",
        "drive_cn=driveに接続\n",
        "if driveに接続:\n",
        "  if not drive._os.path.ismount('/content/drive'):\n",
        "    try:\n",
        "      drive.mount('/content/drive')\n",
        "    except:\n",
        "      print(\"ドライブの接続に失敗しました。\")\n",
        "      drive_cn=False\n",
        "\n",
        "\n",
        "\n",
        "if  drive_cn==False and \"/content/drive/MyDrive\" in 出力するフォルダ:\n",
        "  raise TypeError(\"Googleドライブに接続されていないため、ドライブに保存できません。\")\n",
        "\n",
        "if not os.path.isdir(入力する画像フォルダ):\n",
        "  raise FileNotFoundError(\"画像フォルダが存在しません\")\n",
        "\n",
        "if 出力するフォルダ is None:\n",
        "  raise FileNotFoundError(\"結果を出力するフォルダを入力してください\")\n",
        "\n",
        "\n",
        "\n",
        "os.makedirs(出力するフォルダ,exist_ok=True)\n",
        "\n",
        "\n",
        "try:\n",
        "  import diffusers,transformers\n",
        "except:\n",
        "  !pip install transformers diffusers -q\n",
        "  import transformers,diffusers\n",
        "\n",
        "if not os.path.exists(\"/content/script/diffusers\"):\n",
        "  %cd /content/script\n",
        "  !git clone https://github.com/huggingface/diffusers\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "if \"setup_txt\" not in locals():\n",
        "  %cd /content/script/diffusers/examples/textual_inversion\n",
        "  !pip install -r ./requirements.txt -q\n",
        "  setup_txt=True\n",
        "\n",
        "import accelerate,torchvision,transformers,ftfy ,diffusers\n",
        "\n",
        "\n",
        "%cd /content/diffusers/examples/textual_inversion\n",
        "\n",
        "size=512\n",
        "if モデル名前_or_パス==\"runwayml/stable-diffusion-v1-5\":\n",
        "  size=512\n",
        "else:\n",
        "  size=768\n",
        "\n",
        "#exportは使用不可\n",
        "%env pretrained_model_name_or_path=$モデル名前_or_パス\n",
        "%env train_data_dir=$入力する画像フォルダ\n",
        "%env learnable_property=$概念のタイプ\n",
        "%env placeholder_token=$学習で使用するトークン\n",
        "%env initializer_token=$概念を説明する単語\n",
        "%env resolution=$size\n",
        "%env max_train_steps=$学習ステップ\n",
        "%env output_dir=$出力するフォルダ\n",
        "\n",
        "#--learning_rate=5  \\\n",
        "#--mixed_precision=\"fp16\" \\\n",
        "#learning_rateは整数\n",
        "!accelerate launch textual_inversion.py \\\n",
        "  --pretrained_model_name_or_path=$pretrained_model_name_or_path \\\n",
        "  --train_data_dir=$train_data_dir \\\n",
        "  --learnable_property=$learnable_property \\\n",
        "  --placeholder_token=$placeholder_token \\\n",
        "  --initializer_token=$initializer_token \\\n",
        "  --resolution=$resolution \\\n",
        "  --train_batch_size=4 \\\n",
        "  --gradient_accumulation_steps=4 \\\n",
        "  --max_train_steps=$max_train_steps \\\n",
        "  --lr_scheduler=\"constant\" \\\n",
        "  --learning_rate=5  \\\n",
        "  --lr_warmup_steps=0 \\\n",
        "  --output_dir=$output_dir\n",
        "\n",
        "base=os.path.join(出力するフォルダ,\"learned_embeds.safetensors\")\n",
        "if not 出力ファイル名==\"\":\n",
        "  after_name=出力ファイル名+\".safetensors\"\n",
        "  after=os.path.join(出力するフォルダ,after_name)\n",
        "  if os.path.exists(base):\n",
        "    os.rename(base,after)\n",
        "  else:\n",
        "    print(\"埋め込みファイルが見つかりませんでした。\")\n",
        "else:\n",
        "  after=base\n",
        "\n",
        "print(f\"\"\"\\033[34m\n",
        "学習が終了しました\n",
        "path: {after}\\033[0m\n",
        "\"\"\")\n",
        "\n",
        "if 終了したらランタイムを切断:\n",
        "  from google.colab import runtime\n",
        "  print(\"ランタイムを切断中...\")\n",
        "  runtime.unassign()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OJteaxBCoPKM"
      },
      "source": [
        "#説明書"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "P3nODsN-mW21"
      },
      "outputs": [],
      "source": [
        "#@title 埋め込みを適用 {display-mode: \"form\"}\n",
        "\n",
        "\n",
        "# @markdown * Repo_id_or_path : hugface_idまたはfile_pathの入力をお願いします。\n",
        "\n",
        "# @markdown * token : 埋め込みを呼び出す場合に\"Prompt\"または\"N_prompt\"のいずれかに入力お願いします。\n",
        "\n",
        "# @markdown * weight_name : Repo_idの場合にファイル名前を指定してください\n",
        "\n",
        "# @markdown  ーstep.3の終了後に使用可能です\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ADhtGAW9qhmv"
      },
      "outputs": [],
      "source": [
        "#@title プロンプトの特殊トークン {display-mode: \"form\"}\n",
        "\n",
        "# @markdown >ランダムワード\n",
        "# @markdown\n",
        "# @markdown * {cat,dog},cute = \"cat , cute\" / \"dog , cute\"\n",
        "# @markdown * color,{blue,red,green} = \"color , blue\" / \"color , red\" / \"color , green\"\n",
        "\n",
        "#@markdown >''=文字のスイッチ\n",
        "\n",
        "#@markdown * man,{boy,girl, ' '} = \"man\" / \"man,boy\" / \"man,girl\"\n",
        "\n",
        "#@markdown >file_nameの特殊トークンについて\n",
        "\n",
        "#@markdown ファイル名にパラメータなどの値を入力できます。\n",
        "\n",
        "#@markdown * {prompt} : Prompt\n",
        "\n",
        "#@markdown * {seed} : seed値\n",
        "\n",
        "#@markdown * {model_name} : model_name\n",
        "\n",
        "#@markdown * {g_scale} : guidance_scale\n",
        "\n",
        "#@markdown * {time} : 現在時刻\n",
        "\n",
        "#@markdown * {number} : 生成した回数"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bDhgcTiB1sbA"
      },
      "outputs": [],
      "source": [
        "#@title \"自動で条件を決めて生成\" の詳細 {display-mode: \"form\"}\n",
        "\n",
        "\n",
        "#@markdown  * 詳細設定を推奨の値に設定\n",
        "\n",
        "#@markdown   * 次の機能をオンにします\n",
        "#@markdown   * 画面に表示\n",
        "#@markdown   * 画像の質を上げるプロントを追加する\n",
        "#@markdown   * プロンプトアシストを使う ( MagicPrompt )\n",
        "#@markdown   * 推奨するネガティブプロントを使用\n",
        "#@markdown   * 条件をメタデーターとして追加する\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EK6-GlDG5Dvx"
      },
      "outputs": [],
      "source": [
        "#@title プロンプトアシスタントの詳細 {display-mode: \"form\"}\n",
        "\n",
        "# @markdown アニメ調の画像に適したアシスタントは \" anime-anything-promptgen-v2 \"\n",
        "\n",
        "# @markdown 多目的のアシスタントは \" MagicPrompt-Stable-Diffusion \"\n",
        "\n",
        "# @markdown 使用しない場合は \" None \" の選択をお願いします"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OIqEx5TDV2IA"
      },
      "outputs": [],
      "source": [
        "#@title 用語の説明 {display-mode: \"form\"}\n",
        "\n",
        "# @markdown * txt2img  : テキストから画像\n",
        "\n",
        "# @markdown * img2img : 画像から画像\n",
        "\n",
        "# @markdown * Inpaint : 書いた画像に色をつける\n",
        "\n",
        "# @markdown * txt2video : テキストから動画\n",
        "\n",
        "# @markdown * safe : より安定した安全な画像の生成(txt2img)\n",
        "\n",
        "# @markdown * seed (\"-1\"以上) / seed値を指定します。0の場合ランダムな数字を割り当てます。\n",
        "\n",
        "# @markdown * guidance_scale (5≦15 推奨\"7.5\") / promptの強さの値です。強すぎるとノイズが発生する一方、弱すぎると絵が崩壊します。\n",
        "\n",
        "# @markdown * history_d (\"rand_new\", \"rand_init\" )  推奨:\"rand_new\n",
        "\n",
        "# @markdown * moment(0.1≦1.0 ) 推奨\"0.8\"\n",
        "\n",
        "# @markdown * momentum_hist(-1.0≦1.0)  推奨\"0.2\"\n",
        "\n",
        "#@markdown * 拡散ステップ(1≦1000 推奨\"50\") / 計算をする回数を指定します。回数を減らすほど生成速度が速くなります\n",
        "\n",
        "# @markdown * 縦・横の大きさ ( 推奨\"512\" ) / 大きくすればするほど生成速度が遅くなります(img2img、Inpaint**以外のみ**)\n",
        "\n",
        "#@markdown  * safety_level(強さ) ： WEAK＜MEDIUM＜STRONG＜MAX\n",
        "\n",
        "#@markdown  * 条件を統一 : プロンプトアシスタントを使用するとき、最初の画像のプロンプトを繰り返し使用します\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EB3pvcv0VZKd"
      },
      "outputs": [],
      "source": [
        "#@title デフォルト(値が未入力の場合) {display-mode: \"form\"}\n",
        "\n",
        "#@markdown >保存する先のパス\n",
        "#@markdown * デフォルトでは /content/Generated_images に保存されます。なければ作るようになっています。\n",
        "#@markdown * ドライブに保存する場合 /content/drive/MyDriveを最初につけてください\n",
        "\n",
        "# @markdown >file_name\n",
        "#@markdown * デフォルトは \"GIMG-{number}\" です\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e0CbxSb2n8Re"
      },
      "outputs": [],
      "source": [
        "#@title 追記 {display-mode: \"form\"}\n",
        "\n",
        "# @markdown 2024.1.27日時点でmoment付きEuler または EulerA において、history_d を \"rand_init\" にすると、1枚目が崩壊するバグがあります。\n",
        "# @markdown なお、通常のEuler系のサンプラーでは起きません\n",
        "\n",
        "#@markdown 条件: moment:0.8、moment_hist:0.6\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BoIXG55qgRsh"
      },
      "source": [
        "#Readme"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MJXQRW_E3AsI"
      },
      "source": [
        ">重要な注意点 :\n",
        "* 商用利用はご遠慮ください。\n",
        "* 画像生成によって起こった問題について、当方は一切責任を負いません。\n",
        "\n",
        ">免責事項:\n",
        "*  使用にあたっては、自己責任でお願いします。\n",
        "*  本ノートブックは予告なく変更・非公開・削除する可能性があります。\n",
        "*  利用規約は予告なく変更する場合があります。\n",
        "*  このモデルは、趣味で作成したものであり、商用利用などは意図していません。\n",
        "*  使用にあたって発生した通信量、電気料金など金銭に関わるものの負担は追い兼ねます\n",
        "\n",
        "*  本プロジェクトを利用することにより生じた一切の問題について、当方は一切責任を負いません。\n",
        "\n",
        "ー本プロジェクトとは、本画像生成ノートブックや、githubのページなどをさします\n",
        "___\n",
        ">本プロジェクトの説明\n",
        "* プログラミングの個人的な学習\n",
        "* Stable Diffusionをベースにした画像生成ノートブックです。\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "謝辞\n",
        "\n",
        "本画像生成ノートブックの作成にあたり、オープンソースのリソースやフリーのツールを使用させていただきました。個人的な利用でしたが、これらのリソースやツールがあったからこそ、本プロジェクトを実現することができました。\n",
        "この場を借りて、オープンソースのコミュニティや、フリーのツールを提供してくださった方々に感謝の意を表します。素晴らしいツールや技術を提供してくださり、本プロジェクトを支援してくださったことに心から感謝いたします。"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "ONEtEJyRGYSL",
        "iIEW0jvHK7I1",
        "a1QPxPSzF_aP",
        "OJteaxBCoPKM",
        "BoIXG55qgRsh"
      ],
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
